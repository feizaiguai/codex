#!/usr/bin/env python3
"""
AIæ–°é—»çˆ¬å–è„šæœ¬
çˆ¬å– AIBase çš„ä»Šæ—¥AIæ–°é—»
"""
import sys
import json
import time
from pathlib import Path
from datetime import datetime, date as date_class

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from chrome_engine import ChromeAutomation


def scrape_ai_news():
    """çˆ¬å–AIæ–°é—»"""
    print("=" * 70)
    print("AIæ–°é—»çˆ¬å– - AIBase (å«è¯¦ç»†å†…å®¹)")
    print("=" * 70)

    browser = ChromeAutomation(headless=False)

    # è®¿é—®æ–°é—»é¡µé¢
    url = "https://www.aibase.com/zh/news"
    print(f"\nğŸŒ è®¿é—®: {url}")
    browser.navigate(url, wait_until="domcontentloaded")
    time.sleep(5)

    # è·å–ä»Šå¤©çš„æ—¥æœŸ
    today = date_class.today()
    today_str = today.strftime("%Y-%m-%d")
    print(f"ğŸ“… ä»Šå¤©æ—¥æœŸ: {today_str}")

    # æˆªå›¾
    browser.screenshot("aibase_news_page.png")

    # ä½¿ç”¨JavaScriptæå–æ–°é—»æ•°æ®
    js_code = f"""
    () => {{
        const results = [];

        // å…ˆè·å–é¡µé¢ä¸Šæ‰€æœ‰é“¾æ¥
        const allLinks = Array.from(document.querySelectorAll('a'));

        // è¿‡æ»¤å‡ºæ–°é—»ç›¸å…³çš„é“¾æ¥
        allLinks.forEach((link, index) => {{
            const href = link.getAttribute('href') || '';
            const text = (link.textContent || link.innerText || '').trim();

            // åªå¤„ç†åŒ…å«/news/çš„é“¾æ¥
            if (href.includes('/news/') && !href.includes('#')) {{
                // è·å–å®Œæ•´URL
                let fullUrl = href;
                if (href.startsWith('/')) {{
                    fullUrl = 'https://www.aibase.com' + href;
                }}

                // æŸ¥æ‰¾æ—¥æœŸ
                let parent = link.parentElement;
                let dateText = '';
                for (let i = 0; i < 4; i++) {{
                    if (parent && parent !== document.body) {{
                        const timeElem = parent.querySelector('time, .date, .time, span[class*="date"], span[class*="time"]');
                        if (timeElem) {{
                            dateText = timeElem.textContent || timeElem.getAttribute('datetime') || '';
                            break;
                        }}
                        parent = parent.parentElement;
                    }} else {{
                        break;
                    }}
                }}

                if (text.length > 5 && text.length < 300) {{
                    results.push({{
                        index: results.length + 1,
                        title: text,
                        url: fullUrl,
                        date: dateText
                    }});
                }}
            }}
        }});

        // å»é‡
        const uniqueResults = [];
        const seenUrls = new Set();
        for (const item of results) {{
            if (!seenUrls.has(item.url)) {{
                seenUrls.add(item.url);
                uniqueResults.push(item);
            }}
        }}

        return uniqueResults.slice(0, 10);  // åªå–å‰10æ¡
    }}
    """

    print("\nğŸ” æå–æ–°é—»åˆ—è¡¨...")
    try:
        news_items = browser.page.evaluate(js_code)
        print(f"âœ“ æå–åˆ° {len(news_items)} æ¡æ–°é—»")
    except Exception as e:
        print(f"âš ï¸ æå–å¤±è´¥: {e}")
        news_items = []

    # çˆ¬å–æ¯æ¡æ–°é—»çš„è¯¦ç»†å†…å®¹
    print("\n" + "=" * 70)
    print("å¼€å§‹çˆ¬å–æ–°é—»è¯¦ç»†å†…å®¹...")
    print("=" * 70)

    for i, item in enumerate(news_items, 1):
        print(f"\n[{i}/{len(news_items)}] {item.get('title', '')[:60]}...")
        print(f"   é“¾æ¥: {item.get('url', '')}")

        try:
            # è®¿é—®æ–°é—»è¯¦æƒ…é¡µ
            browser.page.goto(item['url'], wait_until="domcontentloaded")
            time.sleep(3)

            # æå–æ­£æ–‡å†…å®¹
            content_js = """
            () => {
                // æŸ¥æ‰¾æ–‡ç« æ­£æ–‡
                const contentSelectors = [
                    'article',
                    '.article-content',
                    '.content',
                    '.post-content',
                    '[class*="article-body"]',
                    '[class*="post-body"]',
                    'main p',
                    '.entry-content'
                ];

                let content = '';
                let author = '';
                let publishDate = '';

                // æå–ä½œè€…
                const authorElem = document.querySelector('.author, [class*="author"], .writer, .by');
                if (authorElem) {
                    author = authorElem.textContent || authorElem.innerText || '';
                }

                // æå–å‘å¸ƒæ—¥æœŸ
                const dateElem = document.querySelector('time, .publish-date, [class*="date"], [class*="time"]');
                if (dateElem) {
                    publishDate = dateElem.textContent || dateElem.getAttribute('datetime') || '';
                }

                // æå–æ­£æ–‡
                for (const selector of contentSelectors) {
                    const elem = document.querySelector(selector);
                    if (elem) {
                        const paragraphs = elem.querySelectorAll('p');
                        if (paragraphs.length > 0) {
                            const texts = [];
                            paragraphs.forEach(p => {
                                const text = (p.textContent || p.innerText || '').trim();
                                if (text.length > 10) {
                                    texts.push(text);
                                }
                            });
                            if (texts.length > 0) {
                                content = texts.join('\\n\\n');
                                break;
                            }
                        }
                    }
                }

                // å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–é¡µé¢ä¸»è¦å†…å®¹
                if (!content) {
                    const mainContent = document.querySelector('main, article, .content, #content');
                    if (mainContent) {
                        content = mainContent.textContent || mainContent.innerText || '';
                        content = content.substring(0, 3000);
                    }
                }

                return {
                    content: content.substring(0, 2000),
                    author: author.substring(0, 50),
                    publishDate: publishDate.substring(0, 50)
                };
            }
            """

            content_data = browser.page.evaluate(content_js)
            item['content'] = content_data.get('content', '')
            item['author'] = content_data.get('author', '')
            item['publishDate'] = content_data.get('publishDate', '')

            content_preview = item['content'][:100] if item['content'] else 'æ— å†…å®¹'
            print(f"   âœ“ å†…å®¹é•¿åº¦: {len(item['content'])} å­—ç¬¦")
            print(f"   é¢„è§ˆ: {content_preview}...")

        except Exception as e:
            print(f"   âœ— çˆ¬å–å¤±è´¥: {e}")
            item['content'] = ''
            item['error'] = str(e)

        # è¿”å›åˆ—è¡¨é¡µ
        if i < len(news_items):
            browser.page.go_back()
            time.sleep(2)

    # ä¿å­˜æ•°æ®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSONæ–‡ä»¶
    json_file = f"ai_news_detail_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "scraped_at": datetime.now().isoformat(),
            "today": today_str,
            "total_count": len(news_items),
            "news": news_items
        }, f, ensure_ascii=False, indent=2)

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_text = f"""
{'='*70}
                    AIæ–°é—»è¯¦ç»†æŠ¥å‘Š
{'='*70}

æŠ“å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ–°é—»æ¥æº: AIBase (https://www.aibase.com/zh/news)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
æ–°é—»æ€»æ•°: {len(news_items)} æ¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    for i, item in enumerate(news_items, 1):
        report_text += f"""
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ã€æ–°é—» {i}ã€‘{item.get('title', '')}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
é“¾æ¥: {item.get('url', '')}
æ—¥æœŸ: {item.get('date', '')}
"""

        if item.get('author'):
            report_text += f"ä½œè€…: {item.get('author', '')}\n"

        report_text += f"""
æ­£æ–‡å†…å®¹:
{item.get('content', '(æ— å†…å®¹)')}

"""

    # ä¿å­˜æŠ¥å‘Š
    text_file = f"ai_news_detail_report_{timestamp}.txt"
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 70)
    print("çˆ¬å–å®Œæˆæ‘˜è¦")
    print("=" * 70)
    for i, item in enumerate(news_items, 1):
        has_content = "âœ“" if item.get('content') else "âœ—"
        content_len = len(item.get('content', ''))
        title = item.get('title', '')[:60]
        print(f"{has_content} [{i}] {title} ({content_len} å­—ç¬¦)")

    print(f"\nâœ“ JSONæ•°æ®: {json_file}")
    print(f"âœ“ è¯¦ç»†æŠ¥å‘Š: {text_file}")

    print("\næµè§ˆå™¨ä¿æŒæ‰“å¼€ 10 ç§’...")
    time.sleep(10)

    browser.close()

    return json_file, text_file


if __name__ == "__main__":
    try:
        scrape_ai_news()
    except Exception as e:
        print(f"\nâœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
