#!/usr/bin/env python3
"""
AIçœ¼é•œå•†å“è¯„ä»·çˆ¬å–è„šæœ¬ V3
é‡ç‚¹æœç´¢å¯æ­é…æˆ’æŒ‡ä½¿ç”¨çš„AIçœ¼é•œ
ç›´æ¥æå–é¡µé¢æ‰€æœ‰ç”¨æˆ·åé¦ˆæ–‡æœ¬
"""
import sys
import json
import time
import re
from pathlib import Path
from datetime import datetime

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from chrome_engine import ChromeAutomation


# AIçœ¼é•œå…³é”®è¯
KEYWORDS = [
    "æ™ºèƒ½çœ¼é•œ",
    "ARçœ¼é•œ",
    "Xreal çœ¼é•œ",
    "Rokid çœ¼é•œ",
    "é›·é¸Ÿ çœ¼é•œ",
    "Meta Ray Ban"
]


def extract_all_user_feedback(browser):
    """æå–é¡µé¢æ‰€æœ‰ç”¨æˆ·åé¦ˆ/è¯„ä»·å†…å®¹"""
    print("ğŸ” æå–ç”¨æˆ·åé¦ˆæ•°æ®...")

    js_code = """
    () => {
        const results = [];

        // å…ˆå°è¯•æŸ¥æ‰¾è¯„ä»·ç›¸å…³çš„å®¹å™¨
        const reviewContainers = [
            'div[class*="review"]',
            'div[class*="Review"]',
            'div[class*="comment"]',
            'div[class*="Comment"]',
            'div[class*="rate"]',
            'div[class*="Rate"]',
            '.review-item',
            '.comment-item',
            '[data-review]',
            '[class*="feedback"]'
        ];

        // ç”¨æˆ·åé¦ˆå…³é”®è¯
        const feedbackKeywords = [
            'è´¨é‡', 'æ•ˆæœ', 'ä½¿ç”¨', 'ä½“éªŒ', 'æ¨è', 'ä¸é”™', 'å¥½', 'å¾ˆå¥½',
            'æ–¹ä¾¿', 'å®ç”¨', 'æ¸…æ™°', 'èˆ’é€‚', 'æ»¡æ„', 'å–œæ¬¢', 'æ£’',
            'å·®', 'é—®é¢˜', 'ä¸å¥½', 'å¤±æœ›', 'é€€è´§', 'ä¸€èˆ¬',
            'ä½©æˆ´', 'éŸ³è´¨', 'ç»­èˆª', 'è¿æ¥', 'åŠŸèƒ½', 'å¤–è§‚', 'ä»·æ ¼',
            'å‘è´§', 'ç‰©æµ', 'åŒ…è£…', 'å®¢æœ', 'å”®å', 'æ€§ä»·æ¯”'
        ];

        // é¦–å…ˆä»è¯„ä»·å®¹å™¨ä¸­æå–
        for (const selector of reviewContainers) {
            const elements = document.querySelectorAll(selector);
            elements.forEach(elem => {
                const text = (elem.textContent || elem.innerText || '').trim();
                if (text.length >= 15 && text.length <= 500) {
                    results.push({
                        text: text,
                        source: 'container',
                        selector: selector,
                        className: elem.className || ''
                    });
                }
            });
        }

        // å¦‚æœç»“æœå¤ªå°‘ï¼Œåˆ™å…¨å±€æœç´¢
        if (results.length < 10) {
            const allElements = document.querySelectorAll('p, span, div, li, td');
            allElements.forEach(elem => {
                const text = (elem.textContent || elem.innerText || '').trim();
                if (text.length >= 15 && text.length <= 500) {
                    const hasKeyword = feedbackKeywords.some(kw => text.includes(kw));
                    if (hasKeyword) {
                        const isDuplicate = results.some(r =>
                            text.substring(0, 50) === r.text.substring(0, 50)
                        );
                        if (!isDuplicate) {
                            results.push({
                                text: text,
                                source: 'global',
                                tag: elem.tagName,
                                className: elem.className || ''
                            });
                        }
                    }
                }
            });
        }

        // å»é‡
        const uniqueResults = [];
        const seen = new Set();
        for (const item of results) {
            const key = item.text.substring(0, 50);
            if (!seen.has(key)) {
                seen.add(key);
                uniqueResults.push(item);
            }
        }

        return uniqueResults.slice(0, 50);
    }
    """

    try:
        feedbacks = browser.page.evaluate(js_code)
        print(f"âœ“ æå–åˆ° {len(feedbacks)} æ¡ç”¨æˆ·åé¦ˆ")
        return feedbacks
    except Exception as e:
        print(f"âš ï¸ æå–å¤±è´¥: {e}")
        return []


def extract_product_info(browser):
    """æå–å•†å“ä¿¡æ¯"""
    print("ğŸ“¦ æå–å•†å“ä¿¡æ¯...")

    js_code = """
    () => {
        const results = [];

        // æŸ¥æ‰¾ä»·æ ¼
        const pricePatterns = [
            /Â¥?\\d+\\.?\\d*/,
            /\\d+å…ƒ/,
            /\\$\\s*\\d+/
        ];

        // è·å–é¡µé¢æ ‡é¢˜
        results.push({
            type: 'title',
            text: document.title || 'æ— æ ‡é¢˜'
        });

        // æŸ¥æ‰¾ä»·æ ¼å…ƒç´ 
        const priceSelectors = ['.price', '[class*="price"]', '[class*="Price"]', '.J_price'];
        for (const selector of priceSelectors) {
            const elems = document.querySelectorAll(selector);
            elems.forEach(elem => {
                const text = (elem.textContent || elem.innerText || '').trim();
                if (text && text.length < 50) {
                    results.push({
                        type: 'price',
                        text: text
                    });
                }
            });
        }

        // æŸ¥æ‰¾å•†å“å‚æ•°/è§„æ ¼
        const specSelectors = [
            '[class*="param"]',
            '[class*="spec"]',
            '[class*="detail"]',
            '.parameter'
        ];
        for (const selector of specSelectors) {
            const elems = document.querySelectorAll(selector);
            elems.forEach(elem => {
                const text = (elem.textContent || elem.innerText || '').trim();
                if (text.length > 10 && text.length < 300) {
                    results.push({
                        type: 'spec',
                        text: text
                    });
                }
            });
        }

        return results.slice(0, 30);
    }
    """

    try:
        infos = browser.page.evaluate(js_code)
        print(f"âœ“ æå–åˆ° {len(infos)} æ¡å•†å“ä¿¡æ¯")
        return infos
    except Exception as e:
        print(f"âš ï¸ æå–å¤±è´¥: {e}")
        return []


def scrape_taobao_products(browser, keywords, wait_seconds=120):
    """çˆ¬å–æ·˜å®å•†å“å’Œç”¨æˆ·åé¦ˆ"""
    print("=" * 70)
    print("æ·˜å® AIçœ¼é•œ çˆ¬å–")
    print("=" * 70)

    import urllib.parse

    all_data = {
        "products": [],
        "feedbacks": [],
        "specs": []
    }

    for idx, keyword in enumerate(keywords[:3]):
        print(f"\n{'='*70}")
        print(f"[{idx+1}/3] æœç´¢: {keyword}")
        print(f"{'='*70}")

        encoded = urllib.parse.quote(keyword)
        url = f"https://s.taobao.com/search?q={encoded}"

        try:
            browser.navigate(url, wait_until="domcontentloaded")
            time.sleep(3)
        except Exception as e:
            print(f"âš ï¸ å¯¼èˆªå¤±è´¥: {e}")
            continue

        # é¦–æ¬¡ç­‰å¾…ç™»å½•
        if idx == 0:
            browser.wait_for_login(wait_seconds)

        # æŠ“å–å•†å“åˆ—è¡¨
        selectors = [
            "[class*='Card']",
            ".item",
            "[class*='ProductItem']",
            "div[class*='Item']",
            "a[class*='title']"
        ]

        products = []
        for selector in selectors:
            try:
                items = browser.scrape_all_elements(selector, max_items=20)
                if items:
                    print(f"âœ“ æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")
                    products = items
                    break
            except:
                continue

        # ä¿å­˜å•†å“ä¿¡æ¯
        for p in products:
            text_lower = p['text'].lower()
            is_ring = 'æˆ’æŒ‡' in text_lower or 'ring' in text_lower or 'è§¦æ§' in text_lower or 'æ‰‹åŠ¿' in text_lower

            all_data["products"].append({
                "platform": "æ·˜å®",
                "keyword": keyword,
                "title": p['text'][:200],
                "is_ring_related": is_ring
            })

        # ç‚¹å‡»ç¬¬ä¸€ä¸ªå•†å“
        if products:
            try:
                print("\nğŸ” è¿›å…¥å•†å“è¯¦æƒ…é¡µ...")

                links = browser.page.query_selector_all("a[href*='item.taobao.com'], a[href*='detail.taobao.com']")
                if links:
                    href = links[0].get_attribute("href")
                    if href:
                        print(f"   æ­£åœ¨è®¿é—®: {href[:80]}...")
                        browser.page.goto(href, wait_until="domcontentloaded")
                        time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´

                        print(f"   é¡µé¢æ ‡é¢˜: {browser.get_title()}")
                        browser.screenshot(f"taobao_detail_{idx}.png")

                        # æ»šåŠ¨é¡µé¢
                        print("   å¼€å§‹æ»šåŠ¨é¡µé¢...")
                        browser.scroll_to_bottom(slow=True, max_scrolls=15)
                        time.sleep(3)

                        # å°è¯•ç‚¹å‡»è¯„ä»·æ ‡ç­¾
                        print("   å°è¯•ç‚¹å‡»è¯„ä»·æ ‡ç­¾...")
                        review_tab_selectors = [
                            "a:has-text('è¯„ä»·')",
                            "a:has-text('è¯„è®º')",
                            "[data-tab='reviews']",
                            "li:has-text('è¯„ä»·')",
                            "span:has-text('è¯„ä»·')",
                            "div:has-text('è¯„ä»·')"
                        ]
                        clicked = False
                        for tab in review_tab_selectors:
                            try:
                                elem = browser.page.query_selector(tab)
                                if elem:
                                    print(f"   æ‰¾åˆ°è¯„ä»·æ ‡ç­¾: {tab}")
                                    browser.page.click(tab, timeout=3000)
                                    time.sleep(3)
                                    clicked = True
                                    break
                            except:
                                continue

                        if not clicked:
                            print("   æœªæ‰¾åˆ°è¯„ä»·æ ‡ç­¾ï¼Œå°è¯•ç›´æ¥æå–...")

                        # å†æ¬¡æ»šåŠ¨ç¡®ä¿è¯„ä»·åŠ è½½
                        print("   ç»§ç»­æ»šåŠ¨åŠ è½½è¯„ä»·...")
                        browser.scroll_to_bottom(slow=True, max_scrolls=10)
                        time.sleep(3)

                        # æå–ç”¨æˆ·åé¦ˆ
                        print("   æå–ç”¨æˆ·åé¦ˆ...")
                        feedbacks = extract_all_user_feedback(browser)
                        for f in feedbacks:
                            all_data["feedbacks"].append({
                                "platform": "æ·˜å®",
                                "product": keyword,
                                "content": f.get("text", "")
                            })

                        # æå–å•†å“è§„æ ¼
                        specs = extract_product_info(browser)
                        for s in specs:
                            all_data["specs"].append({
                                "platform": "æ·˜å®",
                                "product": keyword,
                                "type": s.get("type", ""),
                                "content": s.get("text", "")
                            })

                        print(f"âœ“ æ”¶é›†åˆ° {len(feedbacks)} æ¡åé¦ˆ")

                        # è¿”å›
                        browser.page.go_back()
                        time.sleep(3)

            except Exception as e:
                print(f"âš ï¸ è¯¦æƒ…é¡µå¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    return all_data


def scrape_jd_products(browser, keywords, wait_seconds=90):
    """çˆ¬å–äº¬ä¸œå•†å“å’Œç”¨æˆ·åé¦ˆ"""
    print("\n" + "=" * 70)
    print("äº¬ä¸œ AIçœ¼é•œ çˆ¬å–")
    print("=" * 70)

    import urllib.parse

    all_data = {
        "products": [],
        "feedbacks": [],
        "specs": []
    }

    for idx, keyword in enumerate(keywords[:3]):
        print(f"\n{'='*70}")
        print(f"[{idx+1}/3] æœç´¢: {keyword}")
        print(f"{'='*70}")

        encoded = urllib.parse.quote(keyword)
        url = f"https://search.jd.com/Search?keyword={encoded}&enc=utf-8"

        try:
            browser.navigate(url, wait_until="domcontentloaded")
            time.sleep(3)
        except Exception as e:
            print(f"âš ï¸ å¯¼èˆªå¤±è´¥: {e}")
            continue

        # é¦–æ¬¡ç­‰å¾…ç™»å½•
        if idx == 0:
            browser.wait_for_login(wait_seconds)

        # æŠ“å–å•†å“åˆ—è¡¨
        selectors = [
            ".gl-item",
            ".gl-warp .gl-item",
            "[class*='product']",
            "[data-sku]",
            "li[class*='item']"
        ]

        products = []
        for selector in selectors:
            try:
                items = browser.scrape_all_elements(selector, max_items=20)
                if items:
                    print(f"âœ“ æ‰¾åˆ° {len(items)} ä¸ªå…ƒç´ ")
                    products = items
                    break
            except:
                continue

        # ä¿å­˜å•†å“ä¿¡æ¯
        for p in products:
            text_lower = p['text'].lower()
            is_ring = 'æˆ’æŒ‡' in text_lower or 'ring' in text_lower or 'è§¦æ§' in text_lower or 'æ‰‹åŠ¿' in text_lower

            all_data["products"].append({
                "platform": "äº¬ä¸œ",
                "keyword": keyword,
                "title": p['text'][:200],
                "is_ring_related": is_ring
            })

        # ç‚¹å‡»ç¬¬ä¸€ä¸ªå•†å“
        if products:
            try:
                print("\nğŸ” è¿›å…¥å•†å“è¯¦æƒ…é¡µ...")

                links = browser.page.query_selector_all(".gl-item a, .p-name a")
                if links:
                    href = links[0].get_attribute("href")
                    if href:
                        if href.startswith('/'):
                            href = "https://item.jd.com" + href

                        print(f"   æ­£åœ¨è®¿é—®: {href[:80]}...")
                        browser.page.goto(href, wait_until="domcontentloaded")
                        time.sleep(5)

                        print(f"   é¡µé¢æ ‡é¢˜: {browser.get_title()}")
                        browser.screenshot(f"jd_detail_{idx}.png")

                        # æ»šåŠ¨é¡µé¢
                        print("   å¼€å§‹æ»šåŠ¨é¡µé¢...")
                        browser.scroll_to_bottom(slow=True, max_scrolls=15)
                        time.sleep(3)

                        # å°è¯•ç‚¹å‡»è¯„ä»·æ ‡ç­¾
                        print("   å°è¯•ç‚¹å‡»è¯„ä»·æ ‡ç­¾...")
                        review_tab_selectors = [
                            "a:has-text('è¯„ä»·')",
                            "a:has-text('å•†å“è¯„ä»·')",
                            ".tab-item:has-text('è¯„ä»·')",
                            "#detail-tab-2",
                            "li:has-text('è¯„ä»·')",
                            "div:has-text('è¯„ä»·')"
                        ]
                        clicked = False
                        for tab in review_tab_selectors:
                            try:
                                elem = browser.page.query_selector(tab)
                                if elem:
                                    print(f"   æ‰¾åˆ°è¯„ä»·æ ‡ç­¾: {tab}")
                                    browser.page.click(tab, timeout=3000)
                                    time.sleep(3)
                                    clicked = True
                                    break
                            except:
                                continue

                        if not clicked:
                            print("   æœªæ‰¾åˆ°è¯„ä»·æ ‡ç­¾ï¼Œå°è¯•ç›´æ¥æå–...")

                        # å†æ¬¡æ»šåŠ¨ç¡®ä¿è¯„ä»·åŠ è½½
                        print("   ç»§ç»­æ»šåŠ¨åŠ è½½è¯„ä»·...")
                        browser.scroll_to_bottom(slow=True, max_scrolls=10)
                        time.sleep(3)

                        # æå–ç”¨æˆ·åé¦ˆ
                        print("   æå–ç”¨æˆ·åé¦ˆ...")
                        feedbacks = extract_all_user_feedback(browser)
                        for f in feedbacks:
                            all_data["feedbacks"].append({
                                "platform": "äº¬ä¸œ",
                                "product": keyword,
                                "content": f.get("text", "")
                            })

                        # æå–å•†å“è§„æ ¼
                        specs = extract_product_info(browser)
                        for s in specs:
                            all_data["specs"].append({
                                "platform": "äº¬ä¸œ",
                                "product": keyword,
                                "type": s.get("type", ""),
                                "content": s.get("text", "")
                            })

                        print(f"âœ“ æ”¶é›†åˆ° {len(feedbacks)} æ¡åé¦ˆ")

                        # è¿”å›
                        browser.page.go_back()
                        time.sleep(3)

            except Exception as e:
                print(f"âš ï¸ è¯¦æƒ…é¡µå¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    return all_data


def analyze_and_generate_report(taobao_data, jd_data):
    """åˆ†ææ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š"""
    print("\n" + "=" * 70)
    print("æ•°æ®åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ")
    print("=" * 70)

    # åˆå¹¶æ•°æ®
    all_products = taobao_data.get("products", []) + jd_data.get("products", [])
    all_feedbacks = taobao_data.get("feedbacks", []) + jd_data.get("feedbacks", [])
    all_specs = taobao_data.get("specs", []) + jd_data.get("specs", [])

    # åˆ†æ
    analysis = {
        "total_products": len(all_products),
        "total_feedbacks": len(all_feedbacks),
        "total_specs": len(all_specs),
        "ring_related": sum(1 for p in all_products if p.get("is_ring_related")),
        "by_platform": {
            "æ·˜å®": len(taobao_data.get("products", [])),
            "äº¬ä¸œ": len(jd_data.get("products", []))
        },
        "keywords_stats": {
            "æˆ’æŒ‡": 0, "ring": 0, "è§¦æ§": 0, "æ‰‹åŠ¿": 0,
            "AR": 0, "Xreal": 0, "Rokid": 0, "é›·é¸Ÿ": 0,
            "Meta": 0, "æ™ºèƒ½": 0
        },
        "feedback_categories": {
            "positive": 0,
            "negative": 0,
            "neutral": 0,
            "quality": 0,
            "comfort": 0,
            "battery": 0,
            "connection": 0,
            "price": 0
        }
    }

    # ç»Ÿè®¡å…³é”®è¯
    for p in all_products:
        title = p.get("title", "").lower()
        for kw in analysis["keywords_stats"]:
            if kw.lower() in title:
                analysis["keywords_stats"][kw] += 1

    # åˆ†æåé¦ˆ
    for f in all_feedbacks:
        content = f.get("content", "").lower()

        # æƒ…æ„Ÿåˆ†æ
        if any(w in content for w in ["å¥½", "ä¸é”™", "æ¨è", "æ»¡æ„", "å–œæ¬¢", "æ£’", "æ¸…æ™°", "æ–¹ä¾¿"]):
            analysis["feedback_categories"]["positive"] += 1
        elif any(w in content for w in ["å·®", "ä¸å¥½", "å¤±æœ›", "é€€è´§", "é—®é¢˜"]):
            analysis["feedback_categories"]["negative"] += 1
        else:
            analysis["feedback_categories"]["neutral"] += 1

        # åˆ†ç±»åˆ†æ
        if any(w in content for w in ["è´¨é‡", "åšå·¥", "æè´¨"]):
            analysis["feedback_categories"]["quality"] += 1
        if any(w in content for w in ["èˆ’é€‚", "ä½©æˆ´", "é‡é‡", "è½»"]):
            analysis["feedback_categories"]["comfort"] += 1
        if any(w in content for w in ["ç»­èˆª", "ç”µæ± ", "ç”µé‡", "å……ç”µ"]):
            analysis["feedback_categories"]["battery"] += 1
        if any(w in content for w in ["è¿æ¥", "è“ç‰™", "é…å¯¹"]):
            analysis["feedback_categories"]["connection"] += 1
        if any(w in content for w in ["ä»·æ ¼", "ä¾¿å®œ", "è´µ", "æ€§ä»·æ¯”"]):
            analysis["feedback_categories"]["price"] += 1

    # ç”ŸæˆæŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    report_text = f"""
{'='*70}
                    AIçœ¼é•œå¸‚åœºè°ƒç ”æŠ¥å‘Š
{'='*70}

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸€ã€æ•°æ®æ¦‚è§ˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  æœç´¢å•†å“æ€»æ•°: {analysis['total_products']}
  æ”¶é›†åé¦ˆæ€»æ•°: {analysis['total_feedbacks']}
  å•†å“è§„æ ¼ä¿¡æ¯: {analysis['total_specs']}
  æˆ’æŒ‡ç›¸å…³å•†å“: {analysis['ring_related']}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
äºŒã€å¹³å°åˆ†å¸ƒ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  æ·˜å®: {analysis['by_platform']['æ·˜å®']} ä¸ªå•†å“
  äº¬ä¸œ: {analysis['by_platform']['äº¬ä¸œ']} ä¸ªå•†å“

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸‰ã€å“ç‰Œå…³é”®è¯ç»Ÿè®¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    for kw, count in sorted(analysis['keywords_stats'].items(), key=lambda x: x[1], reverse=True):
        if count > 0:
            report_text += f"  {kw}: {count} æ¬¡\n"

    report_text += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å››ã€ç”¨æˆ·åé¦ˆåˆ†æ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ç§¯æåé¦ˆ: {analysis['feedback_categories']['positive']} æ¡
  æ¶ˆæåé¦ˆ: {analysis['feedback_categories']['negative']} æ¡
  ä¸­æ€§åé¦ˆ: {analysis['feedback_categories']['neutral']} æ¡

  æŒ‰ç±»åˆ«ç»Ÿè®¡:
  - è´¨é‡ç›¸å…³: {analysis['feedback_categories']['quality']} æ¡
  - èˆ’é€‚åº¦ç›¸å…³: {analysis['feedback_categories']['comfort']} æ¡
  - ç»­èˆªç›¸å…³: {analysis['feedback_categories']['battery']} æ¡
  - è¿æ¥ç›¸å…³: {analysis['feedback_categories']['connection']} æ¡
  - ä»·æ ¼ç›¸å…³: {analysis['feedback_categories']['price']} æ¡
"""

    total_fb = sum([analysis['feedback_categories']['positive'],
                   analysis['feedback_categories']['negative'],
                   analysis['feedback_categories']['neutral']])
    if total_fb > 0:
        positive_rate = analysis['feedback_categories']['positive'] / total_fb * 100
        report_text += f"\n  å¥½è¯„ç‡: {positive_rate:.1f}%\n"

    report_text += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
äº”ã€å•†å“ç¤ºä¾‹
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    for i, p in enumerate(all_products[:10], 1):
        ring_mark = "ğŸ”´ æˆ’æŒ‡ç›¸å…³" if p.get('is_ring_related') else "  "
        title = p.get('title', '')[:80]
        report_text += f"{ring_mark} [{i}] {title}...\n"
        report_text += f"      æ¥æº: {p.get('platform', '')} | æœç´¢: {p.get('keyword', '')}\n\n"

    report_text += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å…­ã€ç”¨æˆ·åé¦ˆç²¾é€‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

    # å»é‡åé¦ˆ
    seen = set()
    unique_feedbacks = []
    for f in all_feedbacks:
        key = f.get('content', '')[:50]
        if key not in seen:
            seen.add(key)
            unique_feedbacks.append(f)

    for i, f in enumerate(unique_feedbacks[:15], 1):
        content = f.get('content', '')[:120]
        platform = f.get('platform', '')
        report_text += f"[{i}] [{platform}] {content}...\n\n"

    # ä¿å­˜æ–‡ä»¶
    text_file = f"ai_glasses_report_{timestamp}.txt"
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(report_text)

    json_file = f"ai_glasses_data_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "generated_at": datetime.now().isoformat(),
            "analysis": analysis,
            "products": all_products,
            "feedbacks": all_feedbacks[:50],
            "specs": all_specs[:30]
        }, f, ensure_ascii=False, indent=2)

    # æ‰“å°æŠ¥å‘Š
    print(report_text)
    print(f"\nâœ“ æ–‡æœ¬æŠ¥å‘Š: {text_file}")
    print(f"âœ“ æ•°æ®æ–‡ä»¶: {json_file}")

    return text_file, json_file


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("       AIçœ¼é•œå¸‚åœºè°ƒç ” - ç”¨æˆ·åé¦ˆçˆ¬å– (V3)")
    print("=" * 70)

    browser = ChromeAutomation(headless=False)

    taobao_data = {"products": [], "feedbacks": [], "specs": []}
    jd_data = {"products": [], "feedbacks": [], "specs": []}

    # çˆ¬å–æ·˜å®
    try:
        print("\nå¼€å§‹çˆ¬å–æ·˜å®...")
        taobao_data = scrape_taobao_products(browser, KEYWORDS, wait_seconds=120)
    except Exception as e:
        print(f"æ·˜å®çˆ¬å–å¼‚å¸¸: {e}")

    # çˆ¬å–äº¬ä¸œ
    try:
        print("\nå¼€å§‹çˆ¬å–äº¬ä¸œ...")
        jd_data = scrape_jd_products(browser, KEYWORDS, wait_seconds=90)
    except Exception as e:
        print(f"äº¬ä¸œçˆ¬å–å¼‚å¸¸: {e}")

    # åˆ†æå’ŒæŠ¥å‘Š
    text_file, json_file = analyze_and_generate_report(taobao_data, jd_data)

    browser.screenshot("ai_glasses_final.png")

    print(f"\n{'='*70}")
    print("çˆ¬å–å®Œæˆï¼")
    print(f"{'='*70}")

    print("\næµè§ˆå™¨ä¿æŒæ‰“å¼€ 10 ç§’...")
    time.sleep(10)
    browser.close()


if __name__ == "__main__":
    main()
