#!/usr/bin/env python3
"""
log_analyzer æ¨¡å—
"""


"""
Log Analyzer - æ™ºèƒ½æ—¥å¿—åˆ†æå·¥å…·
æ”¯æŒå¤šç§æ—¥å¿—æ ¼å¼çš„è§£æã€å¼‚å¸¸æ£€æµ‹ã€äº‹ä»¶å…³è”å’Œå¯è§†åŒ–æŠ¥å‘Š
"""
import re
import json
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import List, Dict, Any, Optional, Tuple
import sys
import hashlib


class LogParser:
    """æ—¥å¿—è§£æå™¨ - è‡ªåŠ¨è¯†åˆ«å’Œè§£æå¤šç§æ—¥å¿—æ ¼å¼"""

    # Apache CLFæ ¼å¼: 127.0.0.1 - - [01/Jan/2025:12:00:00 +0000] "GET /api" 200
    CLF_PATTERN = r'(\S+) \S+ \S+ \[([^\]]+)\] "(\S+) (\S+) (\S+)" (\d+)(?: (\d+))?'

    # Syslogæ ¼å¼: Jan 1 12:00:00 host app[123]: message
    SYSLOG_PATTERN = r'(\w+ \d+ \d+:\d+:\d+) (\S+) (\S+)\[(\d+)\]: (.+)'

    # Logfmtæ ¼å¼: level=info timestamp=2025-01-01T12:00:00Z msg="test"
    LOGFMT_PATTERN = r'(\w+)=("?[^"\s]+"?)'

    # Nginxæ ¼å¼
    NGINX_PATTERN = r'(\S+) - (\S+) \[([^\]]+)\] "(\S+) (\S+) (\S+)" (\d+) (\d+) "([^"]*)" "([^"]*)"'

    # Custom application log pattern
    CUSTOM_PATTERN = r'(\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:?\d{2})?)\s+\[?(\w+)\]?\s+(.*)'

    def __init__(self) -> None:
        self.format_detected = None
        self.parse_errors = 0
        self.supported_formats = {
            'json': 'JSONæ ¼å¼',
            'clf': 'Apache CLFæ ¼å¼',
            'syslog': 'Syslogæ ¼å¼',
            'logfmt': 'Logfmtæ ¼å¼',
            'nginx': 'Nginxæ ¼å¼',
            'custom': 'è‡ªå®šä¹‰åº”ç”¨æ—¥å¿—æ ¼å¼',
            'unknown': 'æœªçŸ¥æ ¼å¼'
        }

    def detect_format(self, line: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ—¥å¿—æ ¼å¼"""
        line = line.strip()
        if not line:
            return 'unknown'

        # JSONæ ¼å¼
        if line.startswith('{'):
            try:
                json.loads(line)
                return 'json'
            except:
                pass

        # Nginxæ ¼å¼
        if re.match(self.NGINX_PATTERN, line):
            return 'nginx'

        # Apache CLFæ ¼å¼
        if re.match(self.CLF_PATTERN, line):
            return 'clf'

        # Syslogæ ¼å¼
        if re.match(self.SYSLOG_PATTERN, line):
            return 'syslog'

        # Logfmtæ ¼å¼
        if '=' in line and len(re.findall(self.LOGFMT_PATTERN, line)) >= 2:
            return 'logfmt'

        # Custom application log
        if re.match(self.CUSTOM_PATTERN, line):
            return 'custom'

        return 'unknown'

    def parse_line(self, line: str) -> Optional[Dict[str, Any]]:
        """è§£æå•è¡Œæ—¥å¿—"""
        line = line.strip()
        if not line:
            return None

        # è‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆé¦–æ¬¡ï¼‰
        if not self.format_detected:
            self.format_detected = self.detect_format(line)

        try:
            if self.format_detected == 'json':
                return self._parse_json(line)
            elif self.format_detected == 'clf':
                return self._parse_clf(line)
            elif self.format_detected == 'syslog':
                return self._parse_syslog(line)
            elif self.format_detected == 'logfmt':
                return self._parse_logfmt(line)
            elif self.format_detected == 'nginx':
                return self._parse_nginx(line)
            elif self.format_detected == 'custom':
                return self._parse_custom(line)
            else:
                return self._parse_plain(line)
        except Exception as e:
            self.parse_errors += 1
            return None

    def _parse_json(self, line: str) -> Dict[str, Any]:
        """è§£æJSONæ ¼å¼æ—¥å¿—"""
        data = json.loads(line)
        return {
            'timestamp': self._extract_timestamp(data),
            'level': data.get('level', data.get('severity', 'INFO')).upper(),
            'message': data.get('message', data.get('msg', '')),
            'trace_id': data.get('trace_id', data.get('traceId', '')),
            'span_id': data.get('span_id', data.get('spanId', '')),
            'raw': data
        }

    def _parse_clf(self, line: str) -> Dict[str, Any]:
        """è§£æApache CLFæ ¼å¼æ—¥å¿—"""
        match = re.match(self.CLF_PATTERN, line)
        if match:
            groups = match.groups()
            ip, timestamp, method, path, protocol, status = groups[:6]
            size = groups[6] if len(groups) > 6 and groups[6] else '0'

            return {
                'timestamp': self._parse_clf_timestamp(timestamp),
                'level': 'ERROR' if int(status) >= 500 else 'WARN' if int(status) >= 400 else 'INFO',
                'message': f"{method} {path} {status}",
                'trace_id': '',
                'span_id': '',
                'raw': {
                    'ip': ip,
                    'method': method,
                    'path': path,
                    'status': int(status),
                    'size': int(size)
                }
            }
        return None

    def _parse_nginx(self, line: str) -> Dict[str, Any]:
        """è§£æNginxæ ¼å¼æ—¥å¿—"""
        match = re.match(self.NGINX_PATTERN, line)
        if match:
            ip, user, timestamp, method, path, protocol, status, size, referer, user_agent = match.groups()
            return {
                'timestamp': self._parse_clf_timestamp(timestamp),
                'level': 'ERROR' if int(status) >= 500 else 'WARN' if int(status) >= 400 else 'INFO',
                'message': f"{method} {path} {status}",
                'trace_id': '',
                'span_id': '',
                'raw': {
                    'ip': ip,
                    'method': method,
                    'path': path,
                    'status': int(status),
                    'size': int(size),
                    'referer': referer,
                    'user_agent': user_agent
                }
            }
        return None

    def _parse_syslog(self, line: str) -> Dict[str, Any]:
        """è§£æSyslogæ ¼å¼æ—¥å¿—"""
        match = re.match(self.SYSLOG_PATTERN, line)
        if match:
            timestamp, host, app, pid, message = match.groups()
            return {
                'timestamp': self._parse_syslog_timestamp(timestamp),
                'level': self._detect_level(message),
                'message': message,
                'trace_id': '',
                'span_id': '',
                'raw': {'host': host, 'app': app, 'pid': pid}
            }
        return None

    def _parse_logfmt(self, line: str) -> Dict[str, Any]:
        """è§£æLogfmtæ ¼å¼æ—¥å¿—"""
        pairs = re.findall(self.LOGFMT_PATTERN, line)
        data = {k: v.strip('"') for k, v in pairs}
        return {
            'timestamp': self._parse_iso_timestamp(data.get('timestamp', data.get('time', ''))),
            'level': data.get('level', 'INFO').upper(),
            'message': data.get('msg', data.get('message', '')),
            'trace_id': data.get('trace_id', ''),
            'span_id': data.get('span_id', ''),
            'raw': data
        }

    def _parse_custom(self, line: str) -> Dict[str, Any]:
        """è§£æè‡ªå®šä¹‰åº”ç”¨æ—¥å¿—æ ¼å¼"""
        match = re.match(self.CUSTOM_PATTERN, line)
        if match:
            timestamp, level, message = match.groups()
            return {
                'timestamp': self._parse_iso_timestamp(timestamp),
                'level': level.upper(),
                'message': message,
                'trace_id': '',
                'span_id': '',
                'raw': {}
            }
        return None

    def _parse_plain(self, line: str) -> Dict[str, Any]:
        """è§£æçº¯æ–‡æœ¬æ—¥å¿—"""
        return {
            'timestamp': datetime.now(),
            'level': self._detect_level(line),
            'message': line,
            'trace_id': '',
            'span_id': '',
            'raw': {}
        }

    def _extract_timestamp(self, data: dict) -> datetime:
        """ä»å­—å…¸ä¸­æå–æ—¶é—´æˆ³"""
        for key in ['timestamp', 'time', '@timestamp', 'datetime']:
            if key in data:
                return self._parse_iso_timestamp(str(data[key]))
        return datetime.now()

    def _parse_iso_timestamp(self, ts: str) -> datetime:
        """è§£æISOæ ¼å¼æ—¶é—´æˆ³"""
        try:
            # å¤„ç†å„ç§ISOæ ¼å¼
            ts = ts.replace('Z', '+00:00')
            return datetime.fromisoformat(ts)
        except:
            try:
                # å°è¯•å¸¸è§æ ¼å¼
                return datetime.strptime(ts.split('.')[0], '%Y-%m-%d %H:%M:%S')
            except:
                return datetime.now()

    def _parse_clf_timestamp(self, ts: str) -> datetime:
        """è§£æCLFæ ¼å¼æ—¶é—´æˆ³: 01/Jan/2025:12:00:00 +0000"""
        try:
            return datetime.strptime(ts.split()[0], '%d/%b/%Y:%H:%M:%S')
        except:
            return datetime.now()

    def _parse_syslog_timestamp(self, ts: str) -> datetime:
        """è§£æSyslogæ ¼å¼æ—¶é—´æˆ³: Jan 1 12:00:00"""
        try:
            current_year = datetime.now().year
            return datetime.strptime(f"{current_year} {ts}", '%Y %b %d %H:%M:%S')
        except:
            return datetime.now()

    def _detect_level(self, message: str) -> str:
        """ä»æ¶ˆæ¯ä¸­æ£€æµ‹æ—¥å¿—çº§åˆ«"""
        message_lower = message.lower()
        if any(word in message_lower for word in ['error', 'exception', 'fatal', 'critical']):
            return 'ERROR'
        elif any(word in message_lower for word in ['warn', 'warning']):
            return 'WARN'
        elif any(word in message_lower for word in ['debug', 'trace']):
            return 'DEBUG'
        return 'INFO'

    def get_format_name(self) -> str:
        """è·å–æ£€æµ‹åˆ°çš„æ ¼å¼åç§°"""
        return self.supported_formats.get(self.format_detected, 'æœªçŸ¥æ ¼å¼')


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨ - ä½¿ç”¨AIå’Œç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸"""

    def __init__(self) -> None:
        self.baseline_error_rate = 0
        self.known_errors = set()
        self.anomalies = []
        self.error_patterns = defaultdict(int)

    def detect_error_spike(self, time_series: Dict[datetime, int]) -> List[Dict]:
        """æ£€æµ‹é”™è¯¯çªå¢"""
        if not time_series:
            return []

        values = list(time_series.values())
        self.baseline_error_rate = sum(values) / len(values) if values else 0
        std_dev = self._calculate_std_dev(values)

        spikes = []
        for timestamp, count in time_series.items():
            # ä½¿ç”¨3-sigmaè§„åˆ™æ£€æµ‹å¼‚å¸¸
            threshold = self.baseline_error_rate + (3 * std_dev)
            if count > max(threshold, self.baseline_error_rate * 2):
                spikes.append({
                    'type': 'error_spike',
                    'severity': 'critical',
                    'timestamp': timestamp,
                    'count': count,
                    'baseline': self.baseline_error_rate,
                    'threshold': threshold,
                    'multiplier': count / self.baseline_error_rate if self.baseline_error_rate > 0 else 0
                })

        return spikes

    def detect_new_errors(self, errors: List[str]) -> List[Dict]:
        """æ£€æµ‹æ–°å‡ºç°çš„é”™è¯¯ç±»å‹"""
        new_errors = []
        for error in errors:
            # æå–é”™è¯¯æ¨¡å¼ï¼ˆå»é™¤å˜åŒ–çš„éƒ¨åˆ†å¦‚IDã€æ—¶é—´æˆ³ç­‰ï¼‰
            pattern = self._extract_error_pattern(error)

            if pattern not in self.known_errors:
                new_errors.append({
                    'type': 'new_error',
                    'severity': 'high',
                    'error': error,
                    'pattern': pattern
                })
                self.known_errors.add(pattern)

            self.error_patterns[pattern] += 1

        return new_errors

    def detect_security_threats(self, logs: List[Dict]) -> List[Dict]:
        """æ£€æµ‹å®‰å…¨å¨èƒ"""
        threats = []

        # æ£€æµ‹æš´åŠ›ç ´è§£ï¼ˆçŸ­æ—¶é—´å¤§é‡ç™»å½•å¤±è´¥ï¼‰
        login_failures = defaultdict(list)
        sql_injection_attempts = []
        suspicious_paths = []

        for log in logs:
            message = log['message'].lower()

            # ç™»å½•å¤±è´¥æ£€æµ‹
            if 'login' in message and any(word in message for word in ['fail', 'denied', 'invalid']):
                ip = log['raw'].get('ip', 'unknown')
                login_failures[ip].append(log['timestamp'])

            # SQLæ³¨å…¥æ£€æµ‹
            if any(pattern in message for pattern in ['union select', 'drop table', '1=1', 'or 1=1']):
                sql_injection_attempts.append({
                    'timestamp': log['timestamp'],
                    'message': log['message'],
                    'ip': log['raw'].get('ip', 'unknown')
                })

            # å¯ç–‘è·¯å¾„è®¿é—®
            path = log['raw'].get('path', '')
            if any(suspicious in path.lower() for suspicious in ['../', 'etc/passwd', 'admin', 'phpinfo']):
                suspicious_paths.append({
                    'timestamp': log['timestamp'],
                    'path': path,
                    'ip': log['raw'].get('ip', 'unknown')
                })

        # æš´åŠ›ç ´è§£å¨èƒ
        for ip, timestamps in login_failures.items():
            if len(timestamps) > 10:
                time_window = (max(timestamps) - min(timestamps)).total_seconds() / 60
                threats.append({
                    'type': 'brute_force',
                    'severity': 'critical' if len(timestamps) > 50 else 'high',
                    'ip': ip,
                    'attempts': len(timestamps),
                    'time_window_minutes': time_window,
                    'message': f'å¯èƒ½çš„æš´åŠ›ç ´è§£æ”»å‡»ï¼šIP {ip} åœ¨ {time_window:.1f} åˆ†é’Ÿå†…æœ‰ {len(timestamps)} æ¬¡ç™»å½•å¤±è´¥'
                })

        # SQLæ³¨å…¥å¨èƒ
        if sql_injection_attempts:
            threats.append({
                'type': 'sql_injection',
                'severity': 'critical',
                'attempts': len(sql_injection_attempts),
                'details': sql_injection_attempts[:5],
                'message': f'æ£€æµ‹åˆ° {len(sql_injection_attempts)} æ¬¡ç–‘ä¼¼SQLæ³¨å…¥æ”»å‡»'
            })

        # å¯ç–‘è·¯å¾„è®¿é—®
        if suspicious_paths:
            threats.append({
                'type': 'path_traversal',
                'severity': 'high',
                'attempts': len(suspicious_paths),
                'details': suspicious_paths[:5],
                'message': f'æ£€æµ‹åˆ° {len(suspicious_paths)} æ¬¡å¯ç–‘è·¯å¾„è®¿é—®'
            })

        return threats

    def detect_performance_anomalies(self, logs: List[Dict]) -> List[Dict]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        anomalies = []
        response_times = []

        for log in logs:
            # ä»æ—¥å¿—ä¸­æå–å“åº”æ—¶é—´
            raw = log.get('raw', {})
            if 'response_time' in raw:
                response_times.append(float(raw['response_time']))
            elif 'duration' in raw:
                response_times.append(float(raw['duration']))

        if response_times:
            avg_time = sum(response_times) / len(response_times)
            std_dev = self._calculate_std_dev(response_times)

            slow_requests = [t for t in response_times if t > avg_time + (3 * std_dev)]

            if slow_requests:
                anomalies.append({
                    'type': 'slow_response',
                    'severity': 'medium',
                    'count': len(slow_requests),
                    'average_time': avg_time,
                    'threshold': avg_time + (3 * std_dev),
                    'message': f'æ£€æµ‹åˆ° {len(slow_requests)} ä¸ªæ…¢å“åº”è¯·æ±‚'
                })

        return anomalies

    def _extract_error_pattern(self, error: str) -> str:
        """æå–é”™è¯¯æ¨¡å¼ï¼ˆå»é™¤å˜åŒ–çš„éƒ¨åˆ†ï¼‰"""
        # ç§»é™¤æ•°å­—ã€UUIDã€æ—¶é—´æˆ³ç­‰
        pattern = re.sub(r'\d+', 'N', error)
        pattern = re.sub(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', 'UUID', pattern)
        pattern = re.sub(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}', 'TIMESTAMP', pattern)
        return pattern

    def _calculate_std_dev(self, values: List[float]) -> float:
        """è®¡ç®—æ ‡å‡†å·®"""
        if not values:
            return 0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5


class EventCorrelator:
    """äº‹ä»¶å…³è”å™¨ - å…³è”åˆ†å¸ƒå¼è¿½è¸ªäº‹ä»¶"""

    def __init__(self) -> None:
        self.traces = defaultdict(list)
        self.event_chains = []

    def correlate_events(self, logs: List[Dict]) -> Dict[str, Any]:
        """å…³è”äº‹ä»¶ï¼Œé‡å»ºè°ƒç”¨é“¾"""
        # æŒ‰trace_idåˆ†ç»„
        for log in logs:
            trace_id = log.get('trace_id', '')
            if trace_id:
                self.traces[trace_id].append(log)

        # æ„å»ºäº‹ä»¶é“¾
        chains = []
        for trace_id, events in self.traces.items():
            if len(events) > 1:
                # æŒ‰æ—¶é—´æ’åº
                events.sort(key=lambda x: x['timestamp'])

                chain = {
                    'trace_id': trace_id,
                    'event_count': len(events),
                    'start_time': events[0]['timestamp'],
                    'end_time': events[-1]['timestamp'],
                    'duration': (events[-1]['timestamp'] - events[0]['timestamp']).total_seconds(),
                    'events': events,
                    'has_error': any(e['level'] in ['ERROR', 'CRITICAL'] for e in events)
                }
                chains.append(chain)

        # æ‰¾å‡ºæœ€é•¿å’Œæœ‰é”™è¯¯çš„è°ƒç”¨é“¾
        chains.sort(key=lambda x: x['duration'], reverse=True)
        error_chains = [c for c in chains if c['has_error']]

        return {
            'total_traces': len(self.traces),
            'correlated_chains': len(chains),
            'longest_chains': chains[:5],
            'error_chains': error_chains[:5],
            'avg_chain_length': sum(c['event_count'] for c in chains) / len(chains) if chains else 0
        }

    def build_timeline(self, logs: List[Dict], time_window: int = 60) -> List[Dict]:
        """é‡å»ºæ—¶é—´çº¿ï¼ˆæŒ‰æ—¶é—´çª—å£åˆ†ç»„äº‹ä»¶ï¼‰"""
        if not logs:
            return []

        # æŒ‰æ—¶é—´æ’åº
        sorted_logs = sorted(logs, key=lambda x: x['timestamp'])

        timeline = []
        current_window = None
        current_events = []

        for log in sorted_logs:
            if not current_window:
                current_window = log['timestamp']
                current_events = [log]
            else:
                # æ£€æŸ¥æ˜¯å¦åœ¨åŒä¸€æ—¶é—´çª—å£
                if (log['timestamp'] - current_window).total_seconds() <= time_window:
                    current_events.append(log)
                else:
                    # ä¿å­˜å½“å‰çª—å£
                    timeline.append({
                        'window_start': current_window,
                        'event_count': len(current_events),
                        'error_count': sum(1 for e in current_events if e['level'] in ['ERROR', 'CRITICAL']),
                        'events': current_events
                    })
                    # å¼€å§‹æ–°çª—å£
                    current_window = log['timestamp']
                    current_events = [log]

        # ä¿å­˜æœ€åä¸€ä¸ªçª—å£
        if current_events:
            timeline.append({
                'window_start': current_window,
                'event_count': len(current_events),
                'error_count': sum(1 for e in current_events if e['level'] in ['ERROR', 'CRITICAL']),
                'events': current_events
            })

        return timeline


class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨ - ä¸»åˆ†æå¼•æ“"""

    def __init__(self) -> None:
        self.parser = LogParser()
        self.detector = AnomalyDetector()
        self.correlator = EventCorrelator()
        self.logs = []
        self.stats = {
            'total_lines': 0,
            'parsed_lines': 0,
            'parse_errors': 0,
            'time_range': {'start': None, 'end': None}
        }

    def analyze_file(self, file_path: str, enable_correlation: bool = True) -> Dict[str, Any]:
        """åˆ†ææ—¥å¿—æ–‡ä»¶

        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            enable_correlation: æ˜¯å¦å¯ç”¨äº‹ä»¶å…³è”ï¼ˆåˆ†å¸ƒå¼è¿½è¸ªï¼‰
        """
        path = Path(file_path)

        if not path.exists():
            return {'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}'}

        # è¯»å–å¹¶è§£ææ—¥å¿—
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                self.stats['total_lines'] += 1
                parsed = self.parser.parse_line(line)
                if parsed:
                    self.logs.append(parsed)
                    self.stats['parsed_lines'] += 1

                    # æ›´æ–°æ—¶é—´èŒƒå›´
                    ts = parsed['timestamp']
                    if not self.stats['time_range']['start'] or ts < self.stats['time_range']['start']:
                        self.stats['time_range']['start'] = ts
                    if not self.stats['time_range']['end'] or ts > self.stats['time_range']['end']:
                        self.stats['time_range']['end'] = ts

        self.stats['parse_errors'] = self.parser.parse_errors

        # æ‰§è¡Œåˆ†æ
        return self._generate_report(enable_correlation)

    def _generate_report(self, enable_correlation: bool) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        # ç»Ÿè®¡é”™è¯¯
        error_logs = [log for log in self.logs if log['level'] in ['ERROR', 'CRITICAL', 'FATAL']]

        # æŒ‰æ—¶é—´åˆ†ç»„ç»Ÿè®¡é”™è¯¯ï¼ˆ5åˆ†é’Ÿçª—å£ï¼‰
        error_time_series = defaultdict(int)
        for log in error_logs:
            time_bucket = log['timestamp'].replace(minute=log['timestamp'].minute // 5 * 5, second=0, microsecond=0)
            error_time_series[time_bucket] += 1

        # æå–é”™è¯¯æ¶ˆæ¯
        error_messages = [log['message'] for log in error_logs]

        # å¼‚å¸¸æ£€æµ‹
        error_spikes = self.detector.detect_error_spike(error_time_series)
        new_errors = self.detector.detect_new_errors(error_messages)
        security_threats = self.detector.detect_security_threats(self.logs)
        performance_anomalies = self.detector.detect_performance_anomalies(self.logs)

        # ç»Ÿè®¡æœ€å¸¸è§çš„é”™è¯¯
        error_counter = Counter(error_messages)
        top_errors = error_counter.most_common(10)

        # æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
        level_distribution = Counter(log['level'] for log in self.logs)

        # äº‹ä»¶å…³è”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        correlation_result = None
        timeline = None
        if enable_correlation:
            correlation_result = self.correlator.correlate_events(self.logs)
            # timeline = self.correlator.build_timeline(error_logs, time_window=300)  # 5åˆ†é’Ÿçª—å£

        # é”™è¯¯æ¨¡å¼åˆ†æ
        error_patterns = self._analyze_error_patterns()

        return {
            'summary': {
                'file_format': self.parser.get_format_name(),
                'total_lines': self.stats['total_lines'],
                'parsed_lines': self.stats['parsed_lines'],
                'parse_success_rate': f"{(self.stats['parsed_lines'] / self.stats['total_lines'] * 100):.1f}%" if self.stats['total_lines'] > 0 else '0%',
                'time_range': {
                    'start': self.stats['time_range']['start'].isoformat() if self.stats['time_range']['start'] else None,
                    'end': self.stats['time_range']['end'].isoformat() if self.stats['time_range']['end'] else None,
                    'duration_hours': (self.stats['time_range']['end'] - self.stats['time_range']['start']).total_seconds() / 3600 if self.stats['time_range']['start'] and self.stats['time_range']['end'] else 0
                },
                'error_count': len(error_logs),
                'error_rate': f"{(len(error_logs) / self.stats['parsed_lines'] * 100):.2f}%" if self.stats['parsed_lines'] > 0 else '0%'
            },
            'level_distribution': dict(level_distribution),
            'anomalies': {
                'error_spikes': error_spikes,
                'new_errors': new_errors,
                'security_threats': security_threats,
                'performance_anomalies': performance_anomalies
            },
            'top_errors': [{'message': msg, 'count': count} for msg, count in top_errors],
            'error_patterns': error_patterns,
            'correlation': correlation_result,
            'timeline': timeline,
            'recommendations': self._generate_recommendations(error_spikes, new_errors, security_threats, performance_anomalies)
        }

    def _analyze_error_patterns(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        patterns = defaultdict(int)
        for pattern, count in self.detector.error_patterns.items():
            patterns[pattern] = count

        # æ’åºæ‰¾å‡ºæœ€å¸¸è§çš„é”™è¯¯æ¨¡å¼
        sorted_patterns = sorted(patterns.items(), key=lambda x: x[1], reverse=True)

        return {
            'total_patterns': len(patterns),
            'top_patterns': [{'pattern': p, 'count': c} for p, c in sorted_patterns[:10]]
        }

    def _generate_recommendations(self, spikes, new_errors, threats, perf_anomalies) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []

        if spikes:
            recommendations.append(f"ğŸ”´ æ£€æµ‹åˆ° {len(spikes)} ä¸ªé”™è¯¯çªå¢æ—¶æ®µï¼Œå»ºè®®æ£€æŸ¥å¯¹åº”æ—¶é—´çš„ç³»ç»Ÿè´Ÿè½½å’Œæ•°æ®åº“è¿æ¥")

        if new_errors:
            recommendations.append(f"ğŸŸ  å‘ç° {len(new_errors)} ä¸ªæ–°é”™è¯¯ç±»å‹ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥æ ¹æœ¬åŸå› ")

        if threats:
            for threat in threats:
                if threat['type'] == 'brute_force':
                    recommendations.append(f"ğŸ”’ æ£€æµ‹åˆ°æš´åŠ›ç ´è§£æ”»å‡»ï¼Œå»ºè®®ç«‹å³å°ç¦IPå¹¶å¯ç”¨é€Ÿç‡é™åˆ¶")
                elif threat['type'] == 'sql_injection':
                    recommendations.append(f"ğŸš¨ æ£€æµ‹åˆ°SQLæ³¨å…¥æ”»å‡»ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥è¾“å…¥éªŒè¯å’Œå‚æ•°åŒ–æŸ¥è¯¢")
                elif threat['type'] == 'path_traversal':
                    recommendations.append(f"âš ï¸ æ£€æµ‹åˆ°è·¯å¾„éå†æ”»å‡»ï¼Œå»ºè®®åŠ å¼ºè·¯å¾„éªŒè¯å’Œè®¿é—®æ§åˆ¶")

        if perf_anomalies:
            recommendations.append(f"ğŸ“Š æ£€æµ‹åˆ° {len(perf_anomalies)} ä¸ªæ€§èƒ½å¼‚å¸¸ï¼Œå»ºè®®ä¼˜åŒ–æ…¢æŸ¥è¯¢å’Œå¢åŠ ç¼“å­˜")

        if not recommendations:
            recommendations.append("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸ï¼Œæ—¥å¿—è¿è¡Œæ­£å¸¸")

        return recommendations


def print_report(report: Dict[str, Any], verbose: bool = False) -> Any:
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("=" * 80)
    print("æ—¥å¿—åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()

    # æ¦‚è¦ä¿¡æ¯
    summary = report['summary']
    print(f"ğŸ“Š æ¦‚è¦ä¿¡æ¯:")
    print(f"   æ—¥å¿—æ ¼å¼: {summary['file_format']}")
    print(f"   æ€»è¡Œæ•°: {summary['total_lines']:,}")
    print(f"   è§£ææˆåŠŸ: {summary['parsed_lines']:,} ({summary['parse_success_rate']})")
    print(f"   æ—¶é—´èŒƒå›´: {summary['time_range']['start']} è‡³ {summary['time_range']['end']}")
    print(f"   æ—¶é—´è·¨åº¦: {summary['time_range']['duration_hours']:.2f} å°æ—¶")
    print(f"   é”™è¯¯æ•°é‡: {summary['error_count']:,} ({summary['error_rate']})")
    print()

    # æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
    print(f"ğŸ“ˆ æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:")
    for level, count in sorted(report['level_distribution'].items()):
        print(f"   {level}: {count:,}")
    print()

    # å¼‚å¸¸æ£€æµ‹
    anomalies = report['anomalies']

    if anomalies['error_spikes']:
        print(f"ğŸ”´ é”™è¯¯çªå¢ ({len(anomalies['error_spikes'])} ä¸ª):")
        for spike in anomalies['error_spikes'][:5]:
            print(f"   æ—¶é—´: {spike['timestamp']}")
            print(f"   é”™è¯¯æ•°: {spike['count']} (åŸºçº¿: {spike['baseline']:.1f}, é˜ˆå€¼: {spike['threshold']:.1f})")
        print()

    if anomalies['new_errors']:
        print(f"ğŸŸ  æ–°é”™è¯¯ç±»å‹ ({len(anomalies['new_errors'])} ä¸ª):")
        for error in anomalies['new_errors'][:5]:
            print(f"   {error['error'][:100]}")
        print()

    if anomalies['security_threats']:
        print(f"ğŸ”’ å®‰å…¨å¨èƒ ({len(anomalies['security_threats'])} ä¸ª):")
        for threat in anomalies['security_threats']:
            print(f"   [{threat['severity'].upper()}] {threat['message']}")
        print()

    if anomalies['performance_anomalies']:
        print(f"ğŸ“Š æ€§èƒ½å¼‚å¸¸ ({len(anomalies['performance_anomalies'])} ä¸ª):")
        for anomaly in anomalies['performance_anomalies']:
            print(f"   {anomaly['message']}")
        print()

    # é”™è¯¯æ¨¡å¼
    if report['error_patterns']['top_patterns']:
        print(f"ğŸ” Top é”™è¯¯æ¨¡å¼:")
        for i, pattern in enumerate(report['error_patterns']['top_patterns'][:5], 1):
            print(f"   {i}. [{pattern['count']:,}æ¬¡] {pattern['pattern'][:80]}")
        print()

    # Topé”™è¯¯
    if report['top_errors']:
        print(f"ğŸ“‹ Top 10 é”™è¯¯:")
        for i, error in enumerate(report['top_errors'], 1):
            print(f"   {i}. [{error['count']:,}æ¬¡] {error['message'][:80]}")
        print()

    # äº‹ä»¶å…³è”
    if report.get('correlation'):
        corr = report['correlation']
        print(f"ğŸ”— äº‹ä»¶å…³è”åˆ†æ:")
        print(f"   æ€»è¿½è¸ªæ•°: {corr['total_traces']}")
        print(f"   å…³è”é“¾æ•°: {corr['correlated_chains']}")
        print(f"   å¹³å‡é“¾é•¿: {corr['avg_chain_length']:.1f}")
        if corr['error_chains']:
            print(f"   é”™è¯¯é“¾æ•°: {len(corr['error_chains'])}")
        print()

    # æ—¶é—´çº¿
    if report.get('timeline') and verbose:
        print(f"ğŸ“… é”™è¯¯æ—¶é—´çº¿:")
        for window in report['timeline'][:10]:
            print(f"   {window['window_start']}: {window['error_count']} ä¸ªé”™è¯¯")
        print()

    # å»ºè®®
    print(f"ğŸ’¡ ä¿®å¤å»ºè®®:")
    for rec in report['recommendations']:
        print(f"   {rec}")
    print()


def main() -> Any:
    try:
        """CLIå…¥å£"""
        import argparse
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‡ºé”™: {e}")
        return 1

    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--json', action='store_true', help='è¾“å‡ºJSONæ ¼å¼æŠ¥å‘Š')
    parser.add_argument('--output', '-o', help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    parser.add_argument('--no-correlation', action='store_true', help='ç¦ç”¨äº‹ä»¶å…³è”åˆ†æ')

    args = parser.parse_args()

    print(f"æ­£åœ¨åˆ†ææ—¥å¿—æ–‡ä»¶: {args.file}")
    print()

    analyzer = LogAnalyzer()
    report = analyzer.analyze_file(args.file, enable_correlation=not args.no_correlation)

    if 'error' in report:
        print(f"é”™è¯¯: {report['error']}")
        sys.exit(1)

    if args.json:
        output = json.dumps(report, indent=2, ensure_ascii=False, default=str)
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(output)
            print(f"JSONæŠ¥å‘Šå·²ä¿å­˜è‡³: {args.output}")
        else:
            print(output)
    else:
        print_report(report, verbose=args.verbose)

        if args.output:
            output_file = Path(args.output)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            print(f"JSONæŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    main()
