---
name: 12-log-analyzer-G
description: Log analysis expert for intelligent log parsing and anomaly detection. Supports multi-format auto-recognition (CLF/JSON/Syslog/Custom), AI anomaly detection, event correlation (distributed tracing), timeline reconstruction (incident retrospection), visualization reports. Use for production troubleshooting, performance analysis, security incident investigation.
---

# log-analyzer - æ—¥å¿—åˆ†æä¸“å®¶

**ç‰ˆæœ¬**: 2.0.0
**ä¼˜å…ˆçº§**: P0
**ç±»åˆ«**: è°ƒè¯•ä¸ç›‘æ§

---

## æè¿°

log-analyzeræ˜¯ä¸“ä¸šçš„æ—¥å¿—åˆ†æä¸“å®¶,æ™ºèƒ½è§£æå¤šæºæ—¥å¿—ã€è¯†åˆ«å¼‚å¸¸æ¨¡å¼ã€å…³è”äº‹ä»¶ã€ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šã€‚æ”¯æŒCLFã€JSONã€Syslogã€Logfmtç­‰å¤šç§æ—¥å¿—æ ¼å¼è‡ªåŠ¨è¯†åˆ«ã€‚é€šè¿‡AIé©±åŠ¨çš„æ¨¡å¼è¯†åˆ«å¿«é€Ÿå‘ç°é”™è¯¯ã€æ€§èƒ½å¼‚å¸¸ã€å®‰å…¨å¨èƒã€‚è·¨æœåŠ¡ã€è·¨æ—¶é—´å…³è”æ—¥å¿—äº‹ä»¶,é‡å»ºè¯·æ±‚å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚ç”Ÿæˆè¶‹åŠ¿å›¾ã€çƒ­åŠ›å›¾ã€é”™è¯¯åˆ†å¸ƒç­‰å¯è§†åŒ–æŠ¥å‘Š,å¿«é€Ÿå®šä½ç”Ÿäº§ç¯å¢ƒé—®é¢˜æ ¹å› ã€‚

---

## æ ¸å¿ƒèƒ½åŠ›

1. **æ™ºèƒ½è§£æ**: è‡ªåŠ¨è¯†åˆ«æ—¥å¿—æ ¼å¼(CLF/JSON/Syslog/è‡ªå®šä¹‰),æå–ç»“æ„åŒ–ä¿¡æ¯
2. **å¼‚å¸¸æ£€æµ‹**: AIè¯†åˆ«é”™è¯¯æ¨¡å¼ã€æ€§èƒ½å¼‚å¸¸(æ…¢æŸ¥è¯¢/è¶…æ—¶)ã€å®‰å…¨å¨èƒ(çˆ†ç ´/æ³¨å…¥)
3. **äº‹ä»¶å…³è”**: è·¨æœåŠ¡ã€è·¨æ—¶é—´å…³è”æ—¥å¿—äº‹ä»¶,è¿½è¸ªåˆ†å¸ƒå¼è¯·æ±‚é“¾è·¯
4. **æ—¶é—´çº¿é‡å»º**: é‡å»ºè¯·æ±‚å®Œæ•´ç”Ÿå‘½å‘¨æœŸ,å¯è§†åŒ–è°ƒç”¨å…³ç³»
5. **å¯è§†åŒ–æŠ¥å‘Š**: ç”Ÿæˆè¶‹åŠ¿å›¾ã€é”™è¯¯çƒ­åŠ›å›¾ã€QPSæ›²çº¿ã€å»¶è¿Ÿåˆ†å¸ƒ

---

## Instructions

### æ—¥å¿—è§£æå¼•æ“

#### 1. è‡ªåŠ¨æ ¼å¼è¯†åˆ«

```python
def auto_detect_log_format(log_lines):
    """
    è‡ªåŠ¨æ£€æµ‹æ—¥å¿—æ ¼å¼

    æ”¯æŒæ ¼å¼:
    - Apache CLF: 127.0.0.1 - - [01/Jan/2025:12:00:00 +0000] "GET /api HTTP/1.1" 200 1234
    - JSON: {"timestamp": "2025-01-01T12:00:00Z", "level": "ERROR", ...}
    - Syslog: Jan 1 12:00:00 host app[123]: message
    - Logfmt: level=info timestamp=2025-01-01T12:00:00Z message="User logged in"
    - Custom patterns
    """
    sample = log_lines[:100]  # é‡‡æ ·å‰100è¡Œ

    formats = [
        ('clf', r'^(\S+) \S+ \S+ \[([^\]]+)\] "(\S+ \S+ \S+)" (\d+) (\d+)'),
        ('json', r'^\{.*"timestamp".*"level".*\}$'),
        ('syslog', r'^(\w+ \d+ \d+:\d+:\d+) (\S+) (\S+)\[(\d+)\]: (.*)$'),
        ('logfmt', r'^(\w+=\S+\s*)+$'),
    ]

    for format_name, pattern in formats:
        match_count = sum(1 for line in sample if re.match(pattern, line))
        if match_count / len(sample) > 0.8:
            return format_name

    return 'custom'  # éœ€è¦è‡ªå®šä¹‰è§£æè§„åˆ™
```

#### 2. ç»“æ„åŒ–è§£æ

```python
def parse_log_entry(log_line, log_format):
    """
    å°†æ—¥å¿—è¡Œè§£æä¸ºç»“æ„åŒ–æ•°æ®

    è¾“å‡º:
    {
        'timestamp': datetime,
        'level': 'INFO|WARN|ERROR',
        'message': str,
        'context': dict,  # ä¸Šä¸‹æ–‡ä¿¡æ¯
        'metadata': dict  # å…ƒæ•°æ®
    }
    """
    if log_format == 'json':
        return json.loads(log_line)

    elif log_format == 'clf':
        pattern = r'^(\S+) \S+ \S+ \[([^\]]+)\] "(\S+) (\S+) (\S+)" (\d+) (\d+)'
        match = re.match(pattern, log_line)
        if match:
            ip, timestamp_str, method, path, protocol, status, size = match.groups()
            return {
                'timestamp': parse_clf_timestamp(timestamp_str),
                'level': determine_level_from_status(int(status)),
                'client_ip': ip,
                'method': method,
                'path': path,
                'status_code': int(status),
                'response_size': int(size)
            }

    elif log_format == 'syslog':
        pattern = r'^(\w+ \d+ \d+:\d+:\d+) (\S+) (\S+)\[(\d+)\]: (.*)$'
        match = re.match(pattern, log_line)
        if match:
            timestamp_str, host, app, pid, message = match.groups()
            return {
                'timestamp': parse_syslog_timestamp(timestamp_str),
                'host': host,
                'application': app,
                'pid': int(pid),
                'message': message,
                'level': extract_level_from_message(message)
            }

    return None
```

### å¼‚å¸¸æ¨¡å¼è¯†åˆ«

#### 1. é”™è¯¯æ¨¡å¼æ£€æµ‹

```python
def detect_error_patterns(parsed_logs):
    """
    è¯†åˆ«é”™è¯¯æ¨¡å¼

    æ¨¡å¼ç±»å‹:
    1. é¢‘ç‡çªå¢: é”™è¯¯æ•°çªç„¶å¢åŠ 
    2. æ–°é”™è¯¯: ä¹‹å‰æœªå‡ºç°çš„é”™è¯¯
    3. é”™è¯¯èšç±»: ç›¸ä¼¼é”™è¯¯é›†ä¸­å‡ºç°
    4. çº§è”æ•…éšœ: é”™è¯¯ä¼ æ’­é“¾è·¯
    """
    patterns = []

    # 1. é¢‘ç‡çªå¢æ£€æµ‹
    error_timeline = build_error_timeline(parsed_logs)
    baseline_rate = calculate_baseline_error_rate(error_timeline)

    for window in sliding_windows(error_timeline, window_size='5min'):
        current_rate = window.error_count / window.duration
        if current_rate > baseline_rate * 3:  # 3å€é˜ˆå€¼
            patterns.append({
                'type': 'error_spike',
                'time_range': window.time_range,
                'error_count': window.error_count,
                'baseline': baseline_rate,
                'current_rate': current_rate,
                'increase_factor': current_rate / baseline_rate
            })

    # 2. æ–°é”™è¯¯æ£€æµ‹
    historical_errors = load_historical_error_signatures()
    current_errors = extract_error_signatures(parsed_logs)

    new_errors = current_errors - historical_errors
    if new_errors:
        patterns.append({
            'type': 'new_error_types',
            'count': len(new_errors),
            'errors': list(new_errors),
            'first_occurrence': {
                err: find_first_occurrence(err, parsed_logs)
                for err in new_errors
            }
        })

    # 3. é”™è¯¯èšç±»
    error_clusters = cluster_similar_errors(parsed_logs)
    for cluster in error_clusters:
        if cluster.size > 10:  # é˜ˆå€¼
            patterns.append({
                'type': 'error_cluster',
                'signature': cluster.representative,
                'count': cluster.size,
                'affected_components': cluster.components,
                'sample_logs': cluster.samples[:5]
            })

    return patterns
```

#### 2. æ€§èƒ½å¼‚å¸¸æ£€æµ‹

```python
def detect_performance_anomalies(parsed_logs):
    """
    æ€§èƒ½å¼‚å¸¸æ£€æµ‹

    æ£€æµ‹é¡¹:
    - å“åº”æ—¶é—´å¼‚å¸¸ (P95/P99è¶…è¿‡é˜ˆå€¼)
    - æ…¢æŸ¥è¯¢ (æ•°æ®åº“æŸ¥è¯¢ >1s)
    - è¶…æ—¶äº‹ä»¶ (è¿æ¥è¶…æ—¶ã€è¯»è¶…æ—¶)
    - ååé‡ä¸‹é™
    """
    anomalies = []

    # æå–å“åº”æ—¶é—´æ•°æ®
    response_times = extract_response_times(parsed_logs)

    if response_times:
        p95 = np.percentile(response_times, 95)
        p99 = np.percentile(response_times, 99)

        # å¼‚å¸¸é˜ˆå€¼: P95 > 2s æˆ– P99 > 5s
        if p95 > 2000:
            anomalies.append({
                'type': 'high_p95_latency',
                'p95_ms': p95,
                'threshold_ms': 2000,
                'affected_requests': count_requests_above(response_times, 2000)
            })

        if p99 > 5000:
            anomalies.append({
                'type': 'high_p99_latency',
                'p99_ms': p99,
                'threshold_ms': 5000
            })

    # æ…¢æŸ¥è¯¢æ£€æµ‹
    slow_queries = []
    for log in parsed_logs:
        if 'query_time' in log and log['query_time'] > 1000:
            slow_queries.append({
                'timestamp': log['timestamp'],
                'query': log.get('query', 'N/A'),
                'duration_ms': log['query_time'],
                'database': log.get('database', 'unknown')
            })

    if slow_queries:
        anomalies.append({
            'type': 'slow_queries',
            'count': len(slow_queries),
            'samples': slow_queries[:10],  # Top 10
            'max_duration_ms': max(q['duration_ms'] for q in slow_queries)
        })

    # è¶…æ—¶äº‹ä»¶
    timeout_events = [
        log for log in parsed_logs
        if any(kw in log.get('message', '').lower()
               for kw in ['timeout', 'timed out', 'connection refused'])
    ]

    if timeout_events:
        anomalies.append({
            'type': 'timeout_events',
            'count': len(timeout_events),
            'services': group_by_service(timeout_events),
            'samples': timeout_events[:5]
        })

    return anomalies
```

#### 3. å®‰å…¨å¨èƒæ£€æµ‹

```python
def detect_security_threats(parsed_logs):
    """
    å®‰å…¨å¨èƒæ£€æµ‹

    å¨èƒç±»å‹:
    - æš´åŠ›ç ´è§£ (å¤šæ¬¡å¤±è´¥ç™»å½•)
    - SQLæ³¨å…¥å°è¯•
    - XSSæ”»å‡»
    - è·¯å¾„éå†
    - å¼‚å¸¸è®¿é—®æ¨¡å¼
    """
    threats = []

    # 1. æš´åŠ›ç ´è§£æ£€æµ‹
    failed_logins = defaultdict(list)
    for log in parsed_logs:
        if is_failed_login(log):
            client_ip = log.get('client_ip')
            failed_logins[client_ip].append(log['timestamp'])

    for ip, timestamps in failed_logins.items():
        if len(timestamps) > 10:  # 10æ¬¡å¤±è´¥
            time_span = (max(timestamps) - min(timestamps)).seconds
            if time_span < 300:  # 5åˆ†é’Ÿå†…
                threats.append({
                    'type': 'brute_force_attack',
                    'severity': 'high',
                    'client_ip': ip,
                    'failed_attempts': len(timestamps),
                    'time_span_seconds': time_span,
                    'first_attempt': min(timestamps),
                    'last_attempt': max(timestamps)
                })

    # 2. SQLæ³¨å…¥æ£€æµ‹
    sql_injection_patterns = [
        r"(?i)(union.*select|select.*from|'; drop table)",
        r"(?i)(or 1=1|and 1=1|' or '1'='1)",
        r"(?i)(exec\(|execute\(|script>)"
    ]

    for log in parsed_logs:
        request_uri = log.get('path', '') + log.get('query_string', '')
        for pattern in sql_injection_patterns:
            if re.search(pattern, request_uri):
                threats.append({
                    'type': 'sql_injection_attempt',
                    'severity': 'critical',
                    'client_ip': log.get('client_ip'),
                    'timestamp': log['timestamp'],
                    'uri': request_uri,
                    'pattern_matched': pattern
                })

    # 3. å¼‚å¸¸è®¿é—®æ¨¡å¼
    access_patterns = analyze_access_patterns(parsed_logs)
    for pattern in access_patterns:
        if pattern.is_suspicious:
            threats.append({
                'type': 'suspicious_access_pattern',
                'severity': 'medium',
                'description': pattern.description,
                'client_ip': pattern.ip,
                'indicators': pattern.indicators
            })

    return threats
```

### äº‹ä»¶å…³è”ä¸æ—¶é—´çº¿é‡å»º

#### 1. åˆ†å¸ƒå¼è¿½è¸ªå…³è”

```python
def correlate_distributed_trace(parsed_logs):
    """
    å…³è”åˆ†å¸ƒå¼è¯·æ±‚é“¾è·¯

    åŸºäº:
    - trace_id / request_id
    - correlation_id
    - span_id (OpenTelemetry)
    """
    traces = defaultdict(list)

    for log in parsed_logs:
        trace_id = (
            log.get('trace_id') or
            log.get('request_id') or
            log.get('correlation_id')
        )

        if trace_id:
            traces[trace_id].append(log)

    # é‡å»ºè°ƒç”¨é“¾
    call_chains = []
    for trace_id, logs in traces.items():
        # æŒ‰æ—¶é—´æ’åº
        logs.sort(key=lambda x: x['timestamp'])

        call_chain = {
            'trace_id': trace_id,
            'start_time': logs[0]['timestamp'],
            'end_time': logs[-1]['timestamp'],
            'duration_ms': (logs[-1]['timestamp'] - logs[0]['timestamp']).total_seconds() * 1000,
            'services': list(set(log.get('service') for log in logs)),
            'spans': [
                {
                    'timestamp': log['timestamp'],
                    'service': log.get('service'),
                    'operation': log.get('operation'),
                    'duration_ms': log.get('duration_ms'),
                    'status': log.get('status')
                }
                for log in logs
            ],
            'has_error': any(log.get('level') == 'ERROR' for log in logs)
        }

        call_chains.append(call_chain)

    return call_chains
```

#### 2. æ—¶é—´çº¿å¯è§†åŒ–

```python
def build_timeline_visualization(events):
    """
    æ„å»ºæ—¶é—´çº¿å¯è§†åŒ–

    è¾“å‡ºASCIIæ—¶é—´çº¿:
    14:00 â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”
              â”ƒ       â”ƒ         â”ƒ
              â†“       â†“         â†“
         [Request] [DB Query] [Error]
    """
    timeline = []

    # æŒ‰æ—¶é—´æ’åº
    sorted_events = sorted(events, key=lambda e: e['timestamp'])

    for event in sorted_events:
        timeline.append({
            'time': event['timestamp'].strftime('%H:%M:%S'),
            'type': event['type'],
            'description': event['description'],
            'severity': event.get('severity', 'info')
        })

    # ç”ŸæˆASCIIå¯è§†åŒ–
    ascii_timeline = generate_ascii_timeline(timeline)

    return ascii_timeline
```

### æ™ºèƒ½æ ¹å› åˆ†æ

```python
def perform_root_cause_analysis(error_spike, all_logs):
    """
    æ ¹å› åˆ†æ

    ç­–ç•¥:
    1. æ—¶é—´çº¿åˆ†æ: åœ¨é”™è¯¯æ¿€å¢å‰å‘ç”Ÿäº†ä»€ä¹ˆ
    2. ä¾èµ–åˆ†æ: å“ªäº›ä¾èµ–æœåŠ¡å‡ºç°å¼‚å¸¸
    3. ä»£ç å˜æ›´: æ˜¯å¦æœ‰æœ€è¿‘çš„éƒ¨ç½²
    4. èµ„æºåˆ†æ: CPU/å†…å­˜/ç½‘ç»œæ˜¯å¦å¼‚å¸¸
    """
    analysis = {
        'error_spike': error_spike,
        'root_cause_candidates': []
    }

    # 1. æ—¶é—´çº¿åˆ†æ (é”™è¯¯å‰30åˆ†é’Ÿ)
    pre_error_window = get_logs_before(
        all_logs,
        error_spike['start_time'],
        minutes=30
    )

    # æŸ¥æ‰¾å¼‚å¸¸äº‹ä»¶
    anomalies_before_error = []

    # æ£€æŸ¥éƒ¨ç½²äº‹ä»¶
    deployments = find_deployment_logs(pre_error_window)
    if deployments:
        analysis['root_cause_candidates'].append({
            'type': 'recent_deployment',
            'confidence': 0.85,
            'evidence': deployments,
            'description': f"Deployment {deployments[0]['version']} occurred {calculate_time_diff(deployments[0]['timestamp'], error_spike['start_time'])} before error spike"
        })

    # æ£€æŸ¥ä¾èµ–æœåŠ¡å¼‚å¸¸
    dependency_errors = find_dependency_errors(pre_error_window)
    if dependency_errors:
        analysis['root_cause_candidates'].append({
            'type': 'dependency_failure',
            'confidence': 0.90,
            'evidence': dependency_errors,
            'description': f"{len(dependency_errors)} dependency errors detected before spike"
        })

    # æ£€æŸ¥èµ„æºå¼‚å¸¸
    resource_alerts = find_resource_alerts(pre_error_window)
    if resource_alerts:
        analysis['root_cause_candidates'].append({
            'type': 'resource_exhaustion',
            'confidence': 0.75,
            'evidence': resource_alerts,
            'description': f"Resource alerts: {', '.join(a['type'] for a in resource_alerts)}"
        })

    # 2. æ¨¡å¼åŒ¹é… (å·²çŸ¥é—®é¢˜åº“)
    known_issues = match_known_issue_patterns(error_spike)
    if known_issues:
        analysis['root_cause_candidates'].extend(known_issues)

    # æ’åºå€™é€‰æ ¹å›  (æŒ‰å¯ä¿¡åº¦)
    analysis['root_cause_candidates'].sort(
        key=lambda x: x['confidence'],
        reverse=True
    )

    return analysis
```

---

## è¾“å…¥å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| log_source | string/array | æ˜¯ | - | æ—¥å¿—æ–‡ä»¶è·¯å¾„æˆ–æ—¥å¿—å†…å®¹ |
| log_format | string | å¦ | auto | clf/json/syslog/custom/auto |
| analysis_type | string | å¦ | analyze | parse/analyze/detect_anomaly/correlate/timeline |
| time_range | object | å¦ | null | æ—¶é—´èŒƒå›´è¿‡æ»¤ {start, end} |
| filters | object | å¦ | {} | è¿‡æ»¤æ¡ä»¶ {level, service} |

---

## è¾“å‡ºæ ¼å¼

```typescript
interface LogAnalyzerOutput {
  summary: {
    total_entries: number;
    errors: number;
    warnings: number;
    time_span: string;
  };

  anomalies: Anomaly[];
  error_patterns: ErrorPattern[];
  security_threats: SecurityThreat[];

  timeline: TimelineEvent[];
  distributed_traces: DistributedTrace[];

  root_cause_analysis: RootCauseAnalysis;
  recommendations: Recommendation[];

  visualizations: {
    error_trend_chart: string;      // Base64 PNG
    heatmap: string;
    qps_curve: string;
  };
}

interface Anomaly {
  type: 'error_spike' | 'performance_degradation' | 'resource_exhaustion';
  severity: 'critical' | 'high' | 'medium' | 'low';
  time_range: string;
  description: string;
  metrics: any;
}

interface ErrorPattern {
  signature: string;
  count: number;
  first_occurrence: string;
  last_occurrence: string;
  affected_services: string[];
  sample_logs: string[];
}
```

---


---

## TypeScriptæ¥å£

### åŸºç¡€è¾“å‡ºæ¥å£

æ‰€æœ‰Skillçš„è¾“å‡ºéƒ½ç»§æ‰¿è‡ª`BaseOutput`ç»Ÿä¸€æ¥å£ï¼š

```typescript
interface BaseOutput {
  success: boolean;
  error?: {
    code: string;
    message: string;
    suggestedFix?: string;
  };
  metadata?: {
    requestId: string;
    timestamp: string;
    version: string;
  };
  warnings?: Array<{
    code: string;
    message: string;
    severity: 'low' | 'medium' | 'high';
  }>;
}
```

### è¾“å…¥æ¥å£

```typescript
interface LogAnalyzerInput {
}
```

### è¾“å‡ºæ¥å£

```typescript
interface LogAnalyzerOutput extends BaseOutput {
  success: boolean;          // æ¥è‡ªBaseOutput
  error?: ErrorInfo;         // æ¥è‡ªBaseOutput
  metadata?: Metadata;       // æ¥è‡ªBaseOutput
  warnings?: Warning[];      // æ¥è‡ªBaseOutput

  // ... å…¶ä»–ä¸šåŠ¡å­—æ®µ
}
```

---

## Examples

### ç¤ºä¾‹: åˆ†æç”Ÿäº§ç¯å¢ƒ500é”™è¯¯æ¿€å¢

**è¾“å…¥**:
```json
{
  "log_source": "/var/log/application.log",
  "analysis_type": "analyze",
  "time_range": {
    "start": "2025-01-15T13:00:00Z",
    "end": "2025-01-15T15:00:00Z"
  }
}
```

**è¾“å‡º**:
```markdown
# æ—¥å¿—åˆ†ææŠ¥å‘Š

## ğŸ“Š æ¦‚è§ˆ

**æ—¶é—´èŒƒå›´**: 2025-01-15 13:00 ~ 15:00 (2å°æ—¶)
**æ€»æ—¥å¿—æ¡ç›®**: 15,432
**é”™è¯¯çº§åˆ«åˆ†å¸ƒ**:
- ERROR: 423 (2.7%) âš ï¸ å¼‚å¸¸é«˜ (å¹³å‡: 0.3%)
- WARN: 1,234 (8.0%)
- INFO: 13,775 (89.3%)

**HTTPçŠ¶æ€ç **:
- 500: 387 âš ï¸ **æ¿€å¢300%**
- 504: 36
- 200: 14,928

## ğŸ” æ ¹å› åˆ†æ

### ä¸»è¦é—®é¢˜: å†…å­˜æ³„æ¼å¯¼è‡´OOM

**è¯æ®1**: OutOfMemoryErroré¢‘ç¹å‡ºç°
- å‡ºç°æ¬¡æ•°: 312æ¬¡
- é¦–æ¬¡å‡ºç°: 14:10:23
- è¶‹åŠ¿: æŒ‡æ•°å¢é•¿

**è¯æ®2**: JVMå †å†…å­˜ä½¿ç”¨
```
13:00 - 72% (æ­£å¸¸)
13:30 - 79% (è½»å¾®ä¸Šå‡)
14:00 - 87% (è­¦å‘Š)
14:10 - 94% (ä¸¥é‡)
14:20 - 98% (ä¸´ç•Œ) â†’ OOMé¢‘å‘
```

**è¯æ®3**: çº§è”æ•…éšœé“¾
```
14:10 - å†…å­˜87% â†’ GCé¢‘ç¹
14:12 - ç³»ç»Ÿå˜æ…¢ â†’ è¯·æ±‚è¶…æ—¶
14:15 - Redisè¶…æ—¶ â†’ ç¼“å­˜å¤±æ•ˆ
14:17 - æ•°æ®åº“å‹åŠ› â†’ è¿æ¥æ± è€—å°½
14:20 - æ”¯ä»˜ç½‘å…³è¶…æ—¶ â†’ è®¢å•å¤±è´¥
14:23 - OOMé¢‘å‘ â†’ æœåŠ¡ä¸å¯ç”¨
```

## ğŸ¯ ä¿®å¤å»ºè®®

### ç´§æ€¥æªæ–½ (ç«‹å³)
1. æ»šåŠ¨é‡å¯å®ä¾‹
2. å¢åŠ å †å†…å­˜: 2GB â†’ 4GB
3. å¯ç”¨GCæ—¥å¿—

### çŸ­æœŸä¿®å¤ (24å°æ—¶)
1. Heap dumpåˆ†æ
2. ä¼˜åŒ–è¿æ¥æ± é…ç½®
3. æ·»åŠ æ–­è·¯å™¨

### é•¿æœŸæ”¹è¿› (1-2å‘¨)
1. å®Œå–„ç›‘æ§å‘Šè­¦
2. è‡ªåŠ¨æ‰©å®¹é…ç½®
3. ä»£ç å®¡è®¡
```

---

## Best Practices

### 1. æ—¥å¿—èšåˆæ¶æ„

```yaml
# ELK Stacké…ç½®
logstash:
  inputs:
    - beats: 5044
    - syslog: 5000
  filters:
    - grok: parse_logs
    - mutate: add_fields
  outputs:
    - elasticsearch: localhost:9200

kibana:
  dashboards:
    - error_trends
    - performance_metrics
    - security_threats
```

### 2. å®æ—¶å‘Šè­¦è§„åˆ™

```python
# é…ç½®å‘Šè­¦è§„åˆ™
alert_rules = [
    {
        'name': 'error_spike',
        'condition': 'error_rate > baseline * 3',
        'window': '5min',
        'action': 'send_pagerduty'
    },
    {
        'name': 'slow_queries',
        'condition': 'p95_query_time > 1000ms',
        'window': '10min',
        'action': 'send_slack'
    }
]
```

### 3. æ—¥å¿—é‡‡æ ·ç­–ç•¥

```python
# é«˜æµé‡ç³»ç»Ÿæ—¥å¿—é‡‡æ ·
sampling_config = {
    'info_logs': 0.1,      # 10%é‡‡æ ·
    'warn_logs': 0.5,      # 50%é‡‡æ ·
    'error_logs': 1.0,     # 100%ä¿ç•™
    'debug_logs': 0.01     # 1%é‡‡æ · (ä»…å¼€å‘ç¯å¢ƒ)
}
```

---

## Related Skills

- `debugger`: æ·±åº¦é—®é¢˜è¯Šæ–­
- `system-monitor`: å®æ—¶ç³»ç»Ÿç›‘æ§
- `performance-optimizer`: æ€§èƒ½ä¼˜åŒ–

---

## Version History

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´è¯´æ˜ |
|------|------|---------|
| 2.0.0 | 2025-12-12 | AIå¼‚å¸¸æ£€æµ‹ã€åˆ†å¸ƒå¼è¿½è¸ªã€æ ¹å› åˆ†æ |
| 1.0.0 | 2025-06-01 | åˆå§‹ç‰ˆæœ¬ |
