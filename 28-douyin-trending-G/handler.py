#!/usr/bin/env python3
"""
Douyin Trending Analyzer - æŠ–éŸ³çƒ­æœåˆ†æå™¨

Author: Claude Code Skills Team
Version: 1.0.0
License: MIT
"""

import os
import requests
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field
import urllib3

# ç¦ç”¨SSLè­¦å‘Šï¼ˆè§£å†³Windows SSLéªŒè¯é—®é¢˜ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


@dataclass
class TrendingTopic:
    """æŠ–éŸ³çƒ­æœè¯é¢˜æ•°æ®æ¨¡å‹"""
    rank: int
    title: str
    hot_index: int
    label: str = ""
    url: str = ""
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DouyinTrendingConfig:
    """é…ç½®å‚æ•°"""
    api_key: str = ""
    api_url: str = "https://apis.tianapi.com/douyinhot/index"
    limit: int = 10
    keyword: Optional[str] = None
    include_analysis: bool = True
    timeout: int = 10
    max_retries: int = 3


class DouyinTrendingAnalyzer:
    """æŠ–éŸ³çƒ­æœåˆ†æå™¨æ ¸å¿ƒç±»"""

    def __init__(self, config: DouyinTrendingConfig = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = config or DouyinTrendingConfig()
        self.update_time = None
        self.topics: List[TrendingTopic] = []

    def fetch_trending(self) -> List[Dict]:
        """
        ä»å¤©è¡ŒAPIè·å–æŠ–éŸ³çƒ­æœæ¦œå•

        Returns:
            List[Dict]: çƒ­æœæ¦œå•åŸå§‹æ•°æ®

        Raises:
            Exception: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        api_key = os.environ.get("TIANAPI_KEY") or self.config.api_key
        if not api_key:
            raise ValueError("ç¼ºå°‘ TIANAPI_KEY")
        params = {"key": api_key}

        for attempt in range(self.config.max_retries):
            try:
                print(f"ğŸ“¡ æ­£åœ¨è·å–æŠ–éŸ³çƒ­æœ... (å°è¯• {attempt + 1}/{self.config.max_retries})")

                # æ·»åŠ è¯·æ±‚å¤´å’Œç¦ç”¨SSLéªŒè¯
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                }

                response = requests.get(
                    self.config.api_url,
                    params=params,
                    headers=headers,
                    timeout=self.config.timeout,
                    verify=False  # ç¦ç”¨SSLéªŒè¯è§£å†³Windows SSLé—®é¢˜
                )
                response.raise_for_status()
                data = response.json()

                # æ£€æŸ¥APIå“åº”çŠ¶æ€
                if data.get('code') == 200:
                    result = data.get('result', {})
                    trending_list = result.get('list', [])
                    self.update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                    print(f"âœ… æˆåŠŸè·å– {len(trending_list)} æ¡çƒ­æœ")
                    return trending_list

                else:
                    error_msg = data.get('msg', 'Unknown error')
                    raise Exception(f"APIé”™è¯¯: {error_msg}")

            except requests.Timeout:
                print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1})")
                if attempt == self.config.max_retries - 1:
                    raise Exception("è·å–çƒ­æœå¤±è´¥: è¯·æ±‚è¶…æ—¶")

            except requests.RequestException as e:
                print(f"âš ï¸ ç½‘ç»œé”™è¯¯: {str(e)}")
                if attempt == self.config.max_retries - 1:
                    raise Exception(f"è·å–çƒ­æœå¤±è´¥: {str(e)}")

            except Exception as e:
                raise Exception(f"è·å–çƒ­æœå¤±è´¥: {str(e)}")

        raise Exception("è·å–çƒ­æœå¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")

    def parse_topics(self, raw_data: List[Dict]) -> List[TrendingTopic]:
        """
        è§£æçƒ­æœæ•°æ®

        Args:
            raw_data: APIè¿”å›çš„åŸå§‹æ•°æ®

        Returns:
            List[TrendingTopic]: è§£æåçš„çƒ­æœè¯é¢˜åˆ—è¡¨
        """
        topics = []

        for idx, item in enumerate(raw_data, 1):
            try:
                # è·å–çƒ­æœæ ‡é¢˜
                title = item.get('word', '').strip()
                if not title:
                    continue

                # è·å–çƒ­åº¦æŒ‡æ•°
                hot_index = item.get('hotindex', 0)
                if isinstance(hot_index, str):
                    hot_index = int(hot_index) if hot_index.isdigit() else 0

                # è·å–æ ‡ç­¾
                label_code = item.get('label', 0)
                label = self._get_label_name(label_code)

                # æ„å»ºæŠ–éŸ³æœç´¢URLï¼ˆæŠ–éŸ³æœç´¢é¡µé¢ï¼‰
                # æ³¨æ„ï¼šæŠ–éŸ³çš„æœç´¢URLæ ¼å¼ä¸º https://www.douyin.com/search/{å…³é”®è¯}
                import urllib.parse
                encoded_title = urllib.parse.quote(title)
                url = f"https://www.douyin.com/search/{encoded_title}"

                topic = TrendingTopic(
                    rank=idx,
                    title=title,
                    hot_index=hot_index,
                    label=label,
                    url=url
                )
                topics.append(topic)

            except Exception as e:
                print(f"âš ï¸ è§£æè¯é¢˜å¤±è´¥: {item.get('word', 'Unknown')} - {str(e)}")
                continue

        return topics

    def _get_label_name(self, label_code: int) -> str:
        """è½¬æ¢æ ‡ç­¾ä»£ç ä¸ºä¸­æ–‡åç§°"""
        label_map = {
            0: '',
            1: 'æ–°',
            2: 'çƒ­',
            3: 'çˆ†',
            4: 'æ²¸',
            5: 'æ¨è',
            8: 'è§†é¢‘',
            9: 'æŒ‘æˆ˜',
            16: 'è¯é¢˜',
            17: 'éŸ³ä¹'
        }
        return label_map.get(label_code, '')

    def filter_topics(self, topics: List[TrendingTopic]) -> List[TrendingTopic]:
        """
        æ ¹æ®é…ç½®ç­›é€‰è¯é¢˜

        Args:
            topics: è¯é¢˜åˆ—è¡¨

        Returns:
            List[TrendingTopic]: ç­›é€‰åçš„è¯é¢˜åˆ—è¡¨
        """
        filtered = topics

        # å…³é”®è¯ç­›é€‰
        if self.config.keyword:
            keyword_lower = self.config.keyword.lower()
            filtered = [
                t for t in filtered
                if keyword_lower in t.title.lower()
            ]
            print(f"ğŸ” å…³é”®è¯ç­›é€‰ '{self.config.keyword}': {len(filtered)} æ¡åŒ¹é…")

        # é™åˆ¶æ•°é‡
        filtered = filtered[:self.config.limit]

        return filtered

    def enrich_topic_details(self, topic: TrendingTopic) -> None:
        """
        ä½¿ç”¨15-web-search-G skillä¸ºè¯é¢˜æ·»åŠ è¯¦ç»†ä¿¡æ¯

        è°ƒç”¨15-web-search-Gçš„Python CLIè¿›è¡Œç½‘ç»œæœç´¢

        Args:
            topic: è¯é¢˜å¯¹è±¡
        """
        if not self.config.include_analysis:
            return

        print(f"  ğŸ” æ­£åœ¨æœç´¢: {topic.title}")

        # æ„å»ºæœç´¢æŸ¥è¯¢
        search_query = f"{topic.title} æŠ–éŸ³ æœ€æ–°æ¶ˆæ¯ èƒŒæ™¯ æ–°é—»"

        try:
            import subprocess
            import os

            # æ„å»º15-web-search-Gå‘½ä»¤
            web_search_dir = "C:/Users/bigbao/.codex/skills/15-web-search-G"

            # æ£€æŸ¥15-web-search-Gæ˜¯å¦å­˜åœ¨
            if not os.path.exists(web_search_dir):
                print(f"  âš ï¸ 15-web-search-G skillæœªæ‰¾åˆ°ï¼Œè·³è¿‡è¯¦ç»†æœç´¢")
                topic.details = {
                    'summary': f'"{topic.title}" - è¯¦ç»†æœç´¢åŠŸèƒ½éœ€è¦15-web-search-G skill',
                    'background': 'è¯·å®‰è£…15-web-search-G skillä»¥è·å–è¯¦ç»†èƒŒæ™¯ä¿¡æ¯',
                    'key_points': ['åŸºæœ¬çƒ­æœä¿¡æ¯å·²æ˜¾ç¤º'],
                    'sources': []
                }
                return

            # æ„å»ºå‘½ä»¤
            cmd = [
                "python",
                f"{web_search_dir}/cli.py",
                search_query,
                "--mode", "auto",
                "--max-results", "10",
                "--time-range", "week",
                "--language", "zh",
                "--output", "markdown"
            ]

            # æ‰§è¡Œæœç´¢ï¼ˆè¶…æ—¶15ç§’ï¼‰
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=15,
                cwd=web_search_dir
            )

            if result.returncode == 0:
                # è§£ææœç´¢ç»“æœ
                search_output = result.stdout

                # ç®€å•è§£æè¾“å‡ºï¼Œæå–å…³é”®ä¿¡æ¯
                summary = self._extract_summary(search_output, topic.title)
                key_points = self._extract_key_points(search_output)
                sources = self._extract_sources(search_output)

                topic.details = {
                    'summary': summary,
                    'background': 'é€šè¿‡15-web-search-Gå¤šå¼•æ“æœç´¢è·å–',
                    'key_points': key_points,
                    'sources': sources,
                    'search_query': search_query
                }

                print(f"  âœ… æœç´¢å®Œæˆ: {len(key_points)} ä¸ªè¦ç‚¹")

            else:
                error_msg = result.stderr or "æœªçŸ¥é”™è¯¯"
                print(f"  âš ï¸ æœç´¢å¤±è´¥: {error_msg[:100]}")
                topic.details = {
                    'summary': f'å…³äº "{topic.title}" çš„æœç´¢æš‚æ—¶å¤±è´¥',
                    'background': 'æœç´¢æœåŠ¡æš‚ä¸å¯ç”¨',
                    'key_points': [],
                    'sources': []
                }

        except subprocess.TimeoutExpired:
            print(f"  âš ï¸ æœç´¢è¶…æ—¶ï¼ˆ15ç§’ï¼‰")
            topic.details = {
                'summary': 'æœç´¢è¶…æ—¶',
                'background': 'æœç´¢æ—¶é—´è¿‡é•¿å·²ç»ˆæ­¢',
                'key_points': [],
                'sources': []
            }

        except Exception as e:
            print(f"  âš ï¸ æœç´¢å¼‚å¸¸: {str(e)}")
            topic.details = {
                'summary': 'è¯¦ç»†ä¿¡æ¯æš‚ä¸å¯ç”¨',
                'background': f'æœç´¢é”™è¯¯: {str(e)}',
                'key_points': [],
                'sources': []
            }

    def _extract_summary(self, search_output: str, topic_title: str) -> str:
        """ä»æœç´¢ç»“æœä¸­æå–æ‘˜è¦"""
        lines = search_output.split('\n')

        # å¯»æ‰¾AIç­”æ¡ˆæˆ–æ‘˜è¦éƒ¨åˆ†
        for i, line in enumerate(lines):
            if 'AIç­”æ¡ˆ' in line or 'Perplexity' in line or 'æ‘˜è¦' in line:
                # è¿”å›æ¥ä¸‹æ¥çš„å‡ è¡Œä½œä¸ºæ‘˜è¦
                summary_lines = []
                for j in range(i+1, min(i+6, len(lines))):
                    if lines[j].strip() and not lines[j].startswith('#'):
                        summary_lines.append(lines[j].strip())
                if summary_lines:
                    return ' '.join(summary_lines)[:200]

        return f'å…³äº"{topic_title}"çš„æœ€æ–°åŠ¨æ€'

    def _extract_key_points(self, search_output: str) -> list:
        """ä»æœç´¢ç»“æœä¸­æå–å…³é”®è¦ç‚¹"""
        lines = search_output.split('\n')
        key_points = []

        # æå–åˆ—è¡¨é¡¹
        for line in lines:
            line = line.strip()
            if line.startswith('-') or line.startswith('â€¢'):
                point = line.lstrip('-â€¢').strip()
                if point and len(point) > 10:
                    key_points.append(point)
                    if len(key_points) >= 5:
                        break

        if not key_points:
            key_points = ['ç›¸å…³æ–°é—»å’Œä¿¡æ¯è¯·æŸ¥çœ‹æ¥æºé“¾æ¥']

        return key_points

    def _extract_sources(self, search_output: str) -> list:
        """ä»æœç´¢ç»“æœä¸­æå–ä¿¡æ¯æ¥æº"""
        import re
        sources = []

        # æå–æ‰€æœ‰URL
        urls = re.findall(r'https?://[^\s\)]+', search_output)

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        seen = set()
        for url in urls:
            if url not in seen and len(sources) < 5:
                sources.append(url)
                seen.add(url)

        return sources

    def generate_report(self, topics: List[TrendingTopic]) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š

        Args:
            topics: è¯é¢˜åˆ—è¡¨

        Returns:
            str: Markdownæ ¼å¼çš„æŠ¥å‘Š
        """
        report_lines = []

        # æ ‡é¢˜
        report_lines.append(f"# ğŸµ æŠ–éŸ³å®æ—¶çƒ­æœæ¦œ")
        report_lines.append(f"")
        report_lines.append(f"**æ›´æ–°æ—¶é—´**: {self.update_time}")
        report_lines.append(f"**çƒ­æœæ•°é‡**: {len(topics)} æ¡")
        report_lines.append(f"")
        report_lines.append(f"---")
        report_lines.append(f"")

        # Top Nçƒ­æœ
        report_lines.append(f"## ğŸ”¥ Top {len(topics)} çƒ­æœè¯é¢˜")
        report_lines.append(f"")

        for topic in topics:
            # è¯é¢˜æ ‡é¢˜
            label_emoji = {
                'çƒ­': 'ğŸ”¥',
                'æ–°': 'ğŸ†•',
                'çˆ†': 'ğŸ’¥',
                'æ²¸': 'ğŸŒ¡ï¸',
                'æ¨è': 'â­',
                'è§†é¢‘': 'ğŸ“¹',
                'æŒ‘æˆ˜': 'ğŸ¯',
                'è¯é¢˜': 'ğŸ’¬',
                'éŸ³ä¹': 'ğŸµ'
            }.get(topic.label, '')

            report_lines.append(f"### {topic.rank}. {topic.title} {label_emoji}")
            report_lines.append(f"")
            report_lines.append(f"**çƒ­åº¦æŒ‡æ•°**: {topic.hot_index:,}")

            if topic.label:
                report_lines.append(f"**æ ‡ç­¾**: {topic.label}")

            report_lines.append(f"")

            # è¯¦ç»†ä¿¡æ¯
            if topic.details and self.config.include_analysis:
                details = topic.details

                if details.get('summary'):
                    report_lines.append(f"**è¯é¢˜æ¦‚è¿°**: {details['summary']}")
                    report_lines.append(f"")

                if details.get('background'):
                    report_lines.append(f"**èƒŒæ™¯ä¿¡æ¯**: {details['background']}")
                    report_lines.append(f"")

                if details.get('key_points'):
                    report_lines.append(f"**å…³é”®è¦ç‚¹**:")
                    for point in details['key_points']:
                        report_lines.append(f"- {point}")
                    report_lines.append(f"")

            # æŠ–éŸ³é“¾æ¥
            report_lines.append(f"**ğŸ”— æŠ–éŸ³æœç´¢**: [{topic.title}]({topic.url})")
            report_lines.append(f"")
            report_lines.append(f"---")
            report_lines.append(f"")

        # æ€»ç»“
        report_lines.append(f"## ğŸ“Š æ•°æ®è¯´æ˜")
        report_lines.append(f"")
        report_lines.append(f"- **æ•°æ®æº**: å¤©è¡ŒAPI (æŠ–éŸ³çƒ­æœæ¦œ)")
        report_lines.append(f"- **æ›´æ–°é¢‘ç‡**: å®æ—¶")
        report_lines.append(f"- **æ•°æ®æ—¶æ•ˆ**: {self.update_time}")
        report_lines.append(f"")

        return "\n".join(report_lines)

    def analyze(self) -> str:
        """
        æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹

        Returns:
            str: Markdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š
        """
        try:
            print("ğŸš€ å¼€å§‹æŠ–éŸ³çƒ­æœåˆ†æ...")
            print("")

            # 1. è·å–çƒ­æœæ•°æ®
            raw_data = self.fetch_trending()

            # 2. è§£ææ•°æ®
            all_topics = self.parse_topics(raw_data)
            print(f"ğŸ“‹ è§£æå®Œæˆ: {len(all_topics)} æ¡çƒ­æœ")

            # 3. ç­›é€‰è¯é¢˜
            filtered_topics = self.filter_topics(all_topics)
            print(f"âœ… ç­›é€‰å®Œæˆ: {len(filtered_topics)} æ¡è¯é¢˜")
            print("")

            # 4. ä¸°å¯Œè¯é¢˜è¯¦æƒ… (ä½¿ç”¨WebSearch)
            if self.config.include_analysis:
                print("ğŸ” æ­£åœ¨æœç´¢è¯é¢˜è¯¦ç»†ä¿¡æ¯...")
                for i, topic in enumerate(filtered_topics, 1):
                    print(f"[{i}/{len(filtered_topics)}]", end=" ")
                    self.enrich_topic_details(topic)
                print("")

            # 5. ç”ŸæˆæŠ¥å‘Š
            self.topics = filtered_topics
            report = self.generate_report(filtered_topics)

            print("âœ… åˆ†æå®Œæˆ!")
            print("")

            return report

        except Exception as e:
            error_msg = f"âŒ åˆ†æå¤±è´¥: {str(e)}"
            print(error_msg)
            return f"# âŒ é”™è¯¯\n\n{error_msg}"


def main():
    """
    ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£

    Usage:
        python handler.py                    # é»˜è®¤å‰10å
        python handler.py --limit 5          # å‰5å
        python handler.py --keyword "éŸ³ä¹"   # æœç´¢å…³é”®è¯
    """
    import argparse

    parser = argparse.ArgumentParser(description='æŠ–éŸ³çƒ­æœåˆ†æå™¨')
    parser.add_argument('--limit', type=int, default=10, help='è¿”å›çƒ­æœæ•°é‡ (é»˜è®¤: 10)')
    parser.add_argument('--keyword', type=str, help='å…³é”®è¯ç­›é€‰')
    parser.add_argument('--no-analysis', action='store_true', help='ä¸åŒ…å«è¯¦ç»†åˆ†æ')

    args = parser.parse_args()

    # åˆ›å»ºé…ç½®
    config = DouyinTrendingConfig(
        limit=args.limit,
        keyword=args.keyword,
        include_analysis=not args.no_analysis
    )

    # æ‰§è¡Œåˆ†æ
    analyzer = DouyinTrendingAnalyzer(config)
    report = analyzer.analyze()

    # è¾“å‡ºæŠ¥å‘Š
    print(report)

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = f"douyin_trending_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")


if __name__ == '__main__':
    main()
